{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9d5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "import traceback\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376fa49",
   "metadata": {},
   "source": [
    "## 1. 파일 비교 분석\n",
    "\n",
    "두 디렉토리에 있는 동일한 이름의 SAS 파일들을 비교하여 칼럼과 값의 차이점을 분석합니다.\n",
    "\n",
    "### 비교 대상\n",
    "- **디렉토리 1**: `/home/hashjamm/project_data/disease_network/sas_files/hr_project/matched_date`\n",
    "- **디렉토리 2**: `/home/hashjamm/project_data/disease_network/sas_files/matched`\n",
    "\n",
    "### 비교 항목\n",
    "1. **칼럼 차이**: 각 파일의 칼럼 구조 비교\n",
    "   - dir1에만 있는 칼럼\n",
    "   - dir2에만 있는 칼럼\n",
    "   - 공통 칼럼\n",
    "\n",
    "2. **값 차이**: 공통 칼럼의 데이터 값 비교\n",
    "   - 비교 조건: 행 수가 동일하고 `PERSON_ID` 종류가 동일한 경우에만 수행\n",
    "   - `PERSON_ID` 기준으로 정렬 후 값 비교\n",
    "   - 차이가 있는 칼럼 식별\n",
    "\n",
    "### 출력 결과\n",
    "- 칼럼 차이가 있는 파일 수 및 차이 패턴\n",
    "- 값 차이가 있는 파일 수 (행 수 동일 + person_id 종류 동일)\n",
    "- 행 수가 다른 파일 수\n",
    "- person_id 종류가 다른 파일 수 (행 수 동일)\n",
    "- 칼럼과 값이 모두 동일한 파일 수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d16cc351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디렉토리 1의 파일 수: 1187\n",
      "디렉토리 2의 파일 수: 1187\n",
      "공통 파일 수: 1187\n",
      "\n",
      "공통 파일 목록 (처음 10개): ['matched_a01.sas7bdat', 'matched_a02.sas7bdat', 'matched_a03.sas7bdat', 'matched_a04.sas7bdat', 'matched_a05.sas7bdat', 'matched_a06.sas7bdat', 'matched_a07.sas7bdat', 'matched_a08.sas7bdat', 'matched_a09.sas7bdat', 'matched_a15.sas7bdat']\n"
     ]
    }
   ],
   "source": [
    "# 디렉토리 경로 설정\n",
    "dir1 = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/matched_date'\n",
    "dir2 = '/home/hashjamm/project_data/disease_network/sas_files/matched'\n",
    "\n",
    "# 각 디렉토리의 파일 목록 가져오기\n",
    "files1 = set([f for f in os.listdir(dir1) if f.endswith('.sas7bdat')])\n",
    "files2 = set([f for f in os.listdir(dir2) if f.endswith('.sas7bdat')])\n",
    "\n",
    "# 공통 파일명 찾기\n",
    "common_files = sorted(files1 & files2)\n",
    "\n",
    "print(f\"디렉토리 1의 파일 수: {len(files1)}\")\n",
    "print(f\"디렉토리 2의 파일 수: {len(files2)}\")\n",
    "print(f\"공통 파일 수: {len(common_files)}\")\n",
    "print(f\"\\n공통 파일 목록 (처음 10개): {common_files[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79d5fcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비교 완료: 1187개 파일 처리됨\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 칼럼 차이와 값 차이를 명확히 확인하는 코드\n",
    "dir1 = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/matched_date'\n",
    "dir2 = '/home/hashjamm/project_data/disease_network/sas_files/matched'\n",
    "\n",
    "# 공통 파일 목록 가져오기\n",
    "files1 = set([f for f in os.listdir(dir1) if f.endswith('.sas7bdat')])\n",
    "files2 = set([f for f in os.listdir(dir2) if f.endswith('.sas7bdat')])\n",
    "common_files = sorted(files1 & files2)\n",
    "\n",
    "# 각 파일에 대한 상세 비교 결과 저장\n",
    "detailed_results = []\n",
    "\n",
    "for filename in common_files:\n",
    "    file1_path = os.path.join(dir1, filename)\n",
    "    file2_path = os.path.join(dir2, filename)\n",
    "    \n",
    "    try:\n",
    "        df1 = pd.read_sas(file1_path, encoding='utf-8')\n",
    "        df2 = pd.read_sas(file2_path, encoding='utf-8')\n",
    "        \n",
    "        # 칼럼 차이 확인\n",
    "        cols1 = set(df1.columns)\n",
    "        cols2 = set(df2.columns)\n",
    "        only_in_dir1 = cols1 - cols2\n",
    "        only_in_dir2 = cols2 - cols1\n",
    "        common_cols = cols1 & cols2\n",
    "        \n",
    "        result = {\n",
    "            'filename': filename,\n",
    "            'dir1_cols': sorted(list(cols1)),\n",
    "            'dir2_cols': sorted(list(cols2)),\n",
    "            'only_in_dir1': sorted(list(only_in_dir1)),\n",
    "            'only_in_dir2': sorted(list(only_in_dir2)),\n",
    "            'common_cols': sorted(list(common_cols)),\n",
    "            'has_col_diff': len(only_in_dir1) > 0 or len(only_in_dir2) > 0,\n",
    "            'dir1_rows': len(df1),\n",
    "            'dir2_rows': len(df2)\n",
    "        }\n",
    "        \n",
    "        # person_id 칼럼 확인\n",
    "        person_id_col = None\n",
    "        for col_name in ['PERSON_ID', 'person_id', 'Person_ID']:\n",
    "            if col_name in common_cols:\n",
    "                person_id_col = col_name\n",
    "                break\n",
    "        \n",
    "        # 공통 칼럼이 있고 행 수가 같고 person_id 종류가 같은 경우 값 비교\n",
    "        if len(common_cols) > 0 and len(df1) == len(df2) and person_id_col:\n",
    "            # person_id 종류 확인\n",
    "            person_id1_set = set(df1[person_id_col].dropna().unique())\n",
    "            person_id2_set = set(df2[person_id_col].dropna().unique())\n",
    "            person_id_same = person_id1_set == person_id2_set\n",
    "            \n",
    "            if person_id_same:\n",
    "                # 공통 칼럼만 선택\n",
    "                df1_common = df1[list(common_cols)].copy()\n",
    "                df2_common = df2[list(common_cols)].copy()\n",
    "                \n",
    "                # person_id 기준으로 정렬 후 비교\n",
    "                df1_sorted = df1_common.sort_values(by=[person_id_col] + [c for c in common_cols if c != person_id_col]).reset_index(drop=True)\n",
    "                df2_sorted = df2_common.sort_values(by=[person_id_col] + [c for c in common_cols if c != person_id_col]).reset_index(drop=True)\n",
    "                \n",
    "                # 값 비교\n",
    "                values_equal = df1_sorted.equals(df2_sorted)\n",
    "                \n",
    "                if not values_equal:\n",
    "                    # 어느 칼럼에서 차이가 나는지 확인\n",
    "                    diff_cols = []\n",
    "                    for col in common_cols:\n",
    "                        if not df1_sorted[col].equals(df2_sorted[col]):\n",
    "                            diff_cols.append(col)\n",
    "                    result['value_diff_cols'] = diff_cols\n",
    "                else:\n",
    "                    result['value_diff_cols'] = []\n",
    "                \n",
    "                result['values_equal'] = values_equal\n",
    "                result['rows_equal'] = True\n",
    "                result['person_id_same'] = True\n",
    "            else:\n",
    "                result['values_equal'] = None\n",
    "                result['rows_equal'] = True\n",
    "                result['person_id_same'] = False\n",
    "                result['value_diff_cols'] = None\n",
    "        elif len(common_cols) > 0:\n",
    "            # 행 수가 다른 경우\n",
    "            result['values_equal'] = None\n",
    "            result['rows_equal'] = False\n",
    "            result['person_id_same'] = None\n",
    "            result['value_diff_cols'] = None\n",
    "        else:\n",
    "            # 공통 칼럼이 없는 경우\n",
    "            result['values_equal'] = None\n",
    "            result['rows_equal'] = None\n",
    "            result['person_id_same'] = None\n",
    "            result['value_diff_cols'] = None\n",
    "        \n",
    "        detailed_results.append(result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        detailed_results.append({\n",
    "            'filename': filename,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "print(f\"비교 완료: {len(detailed_results)}개 파일 처리됨\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd2a221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 요약 테이블로 정리\n",
    "summary_list = []\n",
    "for result in detailed_results:\n",
    "    if 'error' not in result:\n",
    "        summary_list.append({\n",
    "            'filename': result['filename'],\n",
    "            'dir1_칼럼수': len(result['dir1_cols']),\n",
    "            'dir2_칼럼수': len(result['dir2_cols']),\n",
    "            '공통_칼럼수': len(result['common_cols']),\n",
    "            'dir1에만있는칼럼': ', '.join(result['only_in_dir1']) if result['only_in_dir1'] else '',\n",
    "            'dir2에만있는칼럼': ', '.join(result['only_in_dir2']) if result['only_in_dir2'] else '',\n",
    "            '칼럼차이': '예' if result['has_col_diff'] else '아니오',\n",
    "            'dir1_행수': result['dir1_rows'],\n",
    "            'dir2_행수': result['dir2_rows'],\n",
    "            '행수동일': '예' if result['dir1_rows'] == result['dir2_rows'] else '아니오',\n",
    "            'person_id동일': '예' if result.get('person_id_same') == True else ('아니오' if result.get('person_id_same') == False else '비교불가'),\n",
    "            '값동일': '예' if result.get('values_equal') == True else ('아니오' if result.get('values_equal') == False else '비교불가'),\n",
    "            '값차이칼럼': ', '.join(result.get('value_diff_cols', [])) if result.get('value_diff_cols') else ''\n",
    "        })\n",
    "    else:\n",
    "        summary_list.append({\n",
    "            'filename': result['filename'],\n",
    "            '오류': result['error']\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d8bbedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "📊 파일 비교 결과 요약\n",
      "================================================================================\n",
      "\n",
      "총 비교 파일 수: 1187개\n",
      "\n",
      "📋 칼럼 차이:\n",
      "   • 칼럼이 다른 파일: 1187개 (100.0%)\n",
      "   • dir1에만 있는 칼럼: ['DTH_CODE1', 'DTH_CODE2', 'DTH_YM', 'index_date', 'index_key_seq', 'matched_id']\n",
      "   • dir2에만 있는 칼럼: ['AGE_GROUP', 'CTRB_PT_TYPE_CD', 'SEX', 'SGG']\n",
      "\n",
      "📊 값 차이 (행 수 동일 + person_id 종류 동일):\n",
      "   • 값이 다른 파일: 0개 (0.0%)\n",
      "   • 행 수가 다른 파일: 0개 (0.0%)\n",
      "   • person_id 종류가 다른 파일 (행 수 동일): 0개 (0.0%)\n",
      "\n",
      "✅ 동일한 파일:\n",
      "   • 칼럼과 값 모두 동일: 0개 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# 간단한 요약 결과\n",
    "print(\"=\" * 80)\n",
    "print(\"📊 파일 비교 결과 요약\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_files = len(detailed_results)\n",
    "col_diff_count = len(summary_df[summary_df['칼럼차이'] == '예'])\n",
    "value_diff_count = len(summary_df[(summary_df['행수동일'] == '예') & (summary_df['person_id동일'] == '예') & (summary_df['값동일'] == '아니오')])\n",
    "identical_count = len(summary_df[(summary_df['칼럼차이'] == '아니오') & (summary_df['행수동일'] == '예') & (summary_df['person_id동일'] == '예') & (summary_df['값동일'] == '예')])\n",
    "row_diff_count = len(summary_df[summary_df['행수동일'] == '아니오'])\n",
    "person_id_diff_count = len(summary_df[(summary_df['행수동일'] == '예') & (summary_df['person_id동일'] == '아니오')])\n",
    "\n",
    "print(f\"\\n총 비교 파일 수: {total_files}개\")\n",
    "print(f\"\\n📋 칼럼 차이:\")\n",
    "print(f\"   • 칼럼이 다른 파일: {col_diff_count}개 ({col_diff_count/total_files*100:.1f}%)\")\n",
    "\n",
    "if col_diff_count > 0:\n",
    "    # 칼럼 차이 패턴 분석\n",
    "    dir1_only_cols = set()\n",
    "    dir2_only_cols = set()\n",
    "    for result in detailed_results:\n",
    "        if 'error' not in result and result['has_col_diff']:\n",
    "            dir1_only_cols.update(result['only_in_dir1'])\n",
    "            dir2_only_cols.update(result['only_in_dir2'])\n",
    "    \n",
    "    print(f\"   • dir1에만 있는 칼럼: {sorted(dir1_only_cols)}\")\n",
    "    print(f\"   • dir2에만 있는 칼럼: {sorted(dir2_only_cols)}\")\n",
    "\n",
    "print(f\"\\n📊 값 차이 (행 수 동일 + person_id 종류 동일):\")\n",
    "print(f\"   • 값이 다른 파일: {value_diff_count}개 ({value_diff_count/total_files*100:.1f}%)\")\n",
    "print(f\"   • 행 수가 다른 파일: {row_diff_count}개 ({row_diff_count/total_files*100:.1f}%)\")\n",
    "print(f\"   • person_id 종류가 다른 파일 (행 수 동일): {person_id_diff_count}개 ({person_id_diff_count/total_files*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n✅ 동일한 파일:\")\n",
    "print(f\"   • 칼럼과 값 모두 동일: {identical_count}개 ({identical_count/total_files*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ee37cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "파일: outcome_fu_1.sas7bdat\n",
      "================================================================================\n",
      "총 행 수: 3311623, 총 열 수: 2\n",
      "\n",
      "칼럼 목록: ['PERSON_ID', 'abb_sick']\n",
      "\n",
      "상위 5개 행:\n",
      "    PERSON_ID abb_sick\n",
      "0  10000025.0      J00\n",
      "1  10000025.0      J01\n",
      "2  10000025.0      J02\n",
      "3  10000025.0      J20\n",
      "4  10000025.0      L08\n",
      "\n",
      "\n",
      "================================================================================\n",
      "파일: outcome_fu_2.sas7bdat\n",
      "================================================================================\n",
      "총 행 수: 5896267, 총 열 수: 2\n",
      "\n",
      "칼럼 목록: ['PERSON_ID', 'abb_sick']\n",
      "\n",
      "상위 5개 행:\n",
      "    PERSON_ID abb_sick\n",
      "0  10000025.0      B08\n",
      "1  10000025.0      D69\n",
      "2  10000025.0      J00\n",
      "3  10000025.0      J01\n",
      "4  10000025.0      J02\n",
      "\n",
      "\n",
      "================================================================================\n",
      "파일: outcome_fu_3.sas7bdat\n",
      "================================================================================\n",
      "총 행 수: 8229879, 총 열 수: 2\n",
      "\n",
      "칼럼 목록: ['PERSON_ID', 'abb_sick']\n",
      "\n",
      "상위 5개 행:\n",
      "    PERSON_ID abb_sick\n",
      "0  10000025.0      B08\n",
      "1  10000025.0      D69\n",
      "2  10000025.0      J00\n",
      "3  10000025.0      J01\n",
      "4  10000025.0      J02\n",
      "\n",
      "\n",
      "================================================================================\n",
      "파일: outcome_fu_4.sas7bdat\n",
      "================================================================================\n",
      "총 행 수: 10550189, 총 열 수: 2\n",
      "\n",
      "칼럼 목록: ['PERSON_ID', 'abb_sick']\n",
      "\n",
      "상위 5개 행:\n",
      "    PERSON_ID abb_sick\n",
      "0  10000025.0      B08\n",
      "1  10000025.0      D69\n",
      "2  10000025.0      H52\n",
      "3  10000025.0      H60\n",
      "4  10000025.0      H68\n",
      "\n",
      "\n",
      "================================================================================\n",
      "파일: outcome_fu_5.sas7bdat\n",
      "================================================================================\n",
      "총 행 수: 12766765, 총 열 수: 2\n",
      "\n",
      "칼럼 목록: ['PERSON_ID', 'abb_sick']\n",
      "\n",
      "상위 5개 행:\n",
      "    PERSON_ID abb_sick\n",
      "0  10000016.0      I10\n",
      "1  10000016.0      K02\n",
      "2  10000016.0      M15\n",
      "3  10000025.0      B08\n",
      "4  10000025.0      D69\n",
      "\n",
      "\n",
      "================================================================================\n",
      "파일: outcome_fu_6.sas7bdat\n",
      "================================================================================\n",
      "총 행 수: 14985335, 총 열 수: 2\n",
      "\n",
      "칼럼 목록: ['PERSON_ID', 'abb_sick']\n",
      "\n",
      "상위 5개 행:\n",
      "    PERSON_ID abb_sick\n",
      "0  10000016.0      I10\n",
      "1  10000016.0      J00\n",
      "2  10000016.0      K02\n",
      "3  10000016.0      M15\n",
      "4  10000025.0      A09\n",
      "\n",
      "\n",
      "================================================================================\n",
      "파일: outcome_fu_7.sas7bdat\n",
      "================================================================================\n",
      "총 행 수: 17058785, 총 열 수: 2\n",
      "\n",
      "칼럼 목록: ['PERSON_ID', 'abb_sick']\n",
      "\n",
      "상위 5개 행:\n",
      "    PERSON_ID abb_sick\n",
      "0  10000016.0      I10\n",
      "1  10000016.0      J00\n",
      "2  10000016.0      K02\n",
      "3  10000016.0      M15\n",
      "4  10000025.0      A09\n",
      "\n",
      "\n",
      "================================================================================\n",
      "파일: outcome_fu_8.sas7bdat\n",
      "================================================================================\n",
      "총 행 수: 19248484, 총 열 수: 2\n",
      "\n",
      "칼럼 목록: ['PERSON_ID', 'abb_sick']\n",
      "\n",
      "상위 5개 행:\n",
      "    PERSON_ID abb_sick\n",
      "0  10000016.0      I10\n",
      "1  10000016.0      J00\n",
      "2  10000016.0      K02\n",
      "3  10000016.0      M15\n",
      "4  10000025.0      A09\n",
      "\n",
      "\n",
      "================================================================================\n",
      "파일: outcome_fu_9.sas7bdat\n",
      "================================================================================\n",
      "총 행 수: 21351055, 총 열 수: 2\n",
      "\n",
      "칼럼 목록: ['PERSON_ID', 'abb_sick']\n",
      "\n",
      "상위 5개 행:\n",
      "    PERSON_ID abb_sick\n",
      "0  10000016.0      D23\n",
      "1  10000016.0      I10\n",
      "2  10000016.0      J00\n",
      "3  10000016.0      K02\n",
      "4  10000016.0      K05\n",
      "\n",
      "\n",
      "================================================================================\n",
      "파일: outcome_fu_10.sas7bdat\n",
      "================================================================================\n",
      "총 행 수: 23319782, 총 열 수: 2\n",
      "\n",
      "칼럼 목록: ['PERSON_ID', 'abb_sick']\n",
      "\n",
      "상위 5개 행:\n",
      "    PERSON_ID abb_sick\n",
      "0  10000016.0      B86\n",
      "1  10000016.0      D23\n",
      "2  10000016.0      F00\n",
      "3  10000016.0      I10\n",
      "4  10000016.0      J00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# outcome_fu_1.sas7bdat ~ outcome_fu_10.sas7bdat 파일들의 상위 5개 행 확인\n",
    "base_dir = '/home/hashjamm/project_data/disease_network/sas_files'\n",
    "\n",
    "for i in range(1, 11):\n",
    "    file_path = os.path.join(base_dir, f'outcome_fu_{i}.sas7bdat')\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(f\"파일: outcome_fu_{i}.sas7bdat\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_sas(file_path, encoding='utf-8')\n",
    "        print(f\"총 행 수: {len(df)}, 총 열 수: {len(df.columns)}\")\n",
    "        print(f\"\\n칼럼 목록: {list(df.columns)}\")\n",
    "        print(\"\\n상위 5개 행:\")\n",
    "        print(df.head(5))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"파일을 찾을 수 없습니다: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "    \n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad33b17",
   "metadata": {},
   "source": [
    "## 2. 누적 파일 검증: fu_{n+1} == dt_year_{n+1} + fu_n\n",
    "\n",
    "`outcome_fu_{n+1}` 파일이 `outcome_dt_year_{n+1}`과 `outcome_fu_n`을 합친 것과 동일한지 확인합니다.\n",
    "\n",
    "### 비교 대상\n",
    "- **fu_2** == dt_year_2 + fu_1\n",
    "- **fu_3** == dt_year_3 + fu_2\n",
    "- **fu_4** == dt_year_4 + fu_3\n",
    "- ...\n",
    "- **fu_10** == dt_year_10 + fu_9\n",
    "\n",
    "### 비교 항목\n",
    "1. **칼럼 비교**: 각 파일의 칼럼 구조가 동일한지 확인\n",
    "2. **행 수 비교**: 합친 결과와 fu_{n+1}의 행 수가 동일한지 확인\n",
    "3. **값 비교**: 정렬 후 데이터 값이 완전히 동일한지 확인\n",
    "\n",
    "### 메모리 최적화\n",
    "- 각 단계에서 `fu_n` 변수를 재사용하여 메모리 사용량 최소화\n",
    "- 비교 완료 후 불필요한 변수 삭제\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d271b563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "fu_1 파일 읽기 중...\n",
      "================================================================================\n",
      "fu_1: 3311623행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "\n",
      "================================================================================\n",
      "n=1: fu_2 == dt_year_2 + fu_1 확인\n",
      "================================================================================\n",
      "dt_year_2: 2584644행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "fu_2: 5896267행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "합친 결과: 5896267행\n",
      "✅ fu_2 == dt_year_2 + fu_1 (동일함)\n",
      "\n",
      "================================================================================\n",
      "n=2: fu_3 == dt_year_3 + fu_2 확인\n",
      "================================================================================\n",
      "dt_year_3: 2333612행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "fu_3: 8229879행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "합친 결과: 8229879행\n",
      "✅ fu_3 == dt_year_3 + fu_2 (동일함)\n",
      "\n",
      "================================================================================\n",
      "n=3: fu_4 == dt_year_4 + fu_3 확인\n",
      "================================================================================\n",
      "dt_year_4: 2320310행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "fu_4: 10550189행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "합친 결과: 10550189행\n",
      "✅ fu_4 == dt_year_4 + fu_3 (동일함)\n",
      "\n",
      "================================================================================\n",
      "n=4: fu_5 == dt_year_5 + fu_4 확인\n",
      "================================================================================\n",
      "dt_year_5: 2216576행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "fu_5: 12766765행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "합친 결과: 12766765행\n",
      "✅ fu_5 == dt_year_5 + fu_4 (동일함)\n",
      "\n",
      "================================================================================\n",
      "n=5: fu_6 == dt_year_6 + fu_5 확인\n",
      "================================================================================\n",
      "dt_year_6: 2218570행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "fu_6: 14985335행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "합친 결과: 14985335행\n",
      "✅ fu_6 == dt_year_6 + fu_5 (동일함)\n",
      "\n",
      "================================================================================\n",
      "n=6: fu_7 == dt_year_7 + fu_6 확인\n",
      "================================================================================\n",
      "dt_year_7: 2073450행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "fu_7: 17058785행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "합친 결과: 17058785행\n",
      "✅ fu_7 == dt_year_7 + fu_6 (동일함)\n",
      "\n",
      "================================================================================\n",
      "n=7: fu_8 == dt_year_8 + fu_7 확인\n",
      "================================================================================\n",
      "dt_year_8: 2189699행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "fu_8: 19248484행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "합친 결과: 19248484행\n",
      "✅ fu_8 == dt_year_8 + fu_7 (동일함)\n",
      "\n",
      "================================================================================\n",
      "n=8: fu_9 == dt_year_9 + fu_8 확인\n",
      "================================================================================\n",
      "dt_year_9: 2102571행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "fu_9: 21351055행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "합친 결과: 21351055행\n",
      "✅ fu_9 == dt_year_9 + fu_8 (동일함)\n",
      "\n",
      "================================================================================\n",
      "n=9: fu_10 == dt_year_10 + fu_9 확인\n",
      "================================================================================\n",
      "dt_year_10: 1968727행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "fu_10: 23319782행, 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "합친 결과: 23319782행\n",
      "✅ fu_10 == dt_year_10 + fu_9 (동일함)\n",
      "\n",
      "================================================================================\n",
      "요약\n",
      "================================================================================\n",
      "성공: 9/9개\n"
     ]
    }
   ],
   "source": [
    "# fu_{n+1} == dt_year_{n+1} + fu_n 확인 (메모리 최적화: 변수 재사용)\n",
    "base_dir = '/home/hashjamm/project_data/disease_network/sas_files'\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "# fu_1 먼저 읽기\n",
    "print(\"=\" * 80)\n",
    "print(\"fu_1 파일 읽기 중...\")\n",
    "print(\"=\" * 80)\n",
    "fu_n_path = os.path.join(base_dir, 'outcome_fu_1.sas7bdat')\n",
    "fu_n = pd.read_sas(fu_n_path, encoding='utf-8')\n",
    "print(f\"fu_1: {len(fu_n)}행, 칼럼: {list(fu_n.columns)}\")\n",
    "\n",
    "# fu_2부터 fu_10까지 확인\n",
    "for n in range(1, 10):\n",
    "    n_plus_1 = n + 1\n",
    "    \n",
    "    # dt_year_{n+1} 읽기\n",
    "    dt_year_path = os.path.join(base_dir, f'outcome_dt_year_{n_plus_1}.sas7bdat')\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"n={n}: fu_{n_plus_1} == dt_year_{n_plus_1} + fu_{n} 확인\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        dt_year_n_plus_1 = pd.read_sas(dt_year_path, encoding='utf-8')\n",
    "        print(f\"dt_year_{n_plus_1}: {len(dt_year_n_plus_1)}행, 칼럼: {list(dt_year_n_plus_1.columns)}\")\n",
    "        \n",
    "        # fu_{n+1} 읽기\n",
    "        fu_n_plus_1_path = os.path.join(base_dir, f'outcome_fu_{n_plus_1}.sas7bdat')\n",
    "        fu_n_plus_1 = pd.read_sas(fu_n_plus_1_path, encoding='utf-8')\n",
    "        print(f\"fu_{n_plus_1}: {len(fu_n_plus_1)}행, 칼럼: {list(fu_n_plus_1.columns)}\")\n",
    "        \n",
    "        # fu_n과 dt_year_{n+1} 합치기\n",
    "        combined = pd.concat([fu_n, dt_year_n_plus_1], ignore_index=True)\n",
    "        print(f\"합친 결과: {len(combined)}행\")\n",
    "        \n",
    "        # 칼럼 비교\n",
    "        cols_fu = set(fu_n_plus_1.columns)\n",
    "        cols_combined = set(combined.columns)\n",
    "        cols_equal = cols_fu == cols_combined\n",
    "        \n",
    "        result = {\n",
    "            'n': n,\n",
    "            'fu_n_plus_1_rows': len(fu_n_plus_1),\n",
    "            'combined_rows': len(combined),\n",
    "            'rows_equal': len(fu_n_plus_1) == len(combined),\n",
    "            'cols_equal': cols_equal,\n",
    "            'fu_n_plus_1_cols': sorted(list(cols_fu)),\n",
    "            'combined_cols': sorted(list(cols_combined))\n",
    "        }\n",
    "        \n",
    "        # 칼럼이 같고 행 수가 같으면 값 비교\n",
    "        if cols_equal and len(fu_n_plus_1) == len(combined):\n",
    "            # 정렬 후 비교\n",
    "            sort_cols = sorted(list(cols_fu))\n",
    "            fu_sorted = fu_n_plus_1[sort_cols].sort_values(by=sort_cols).reset_index(drop=True)\n",
    "            combined_sorted = combined[sort_cols].sort_values(by=sort_cols).reset_index(drop=True)\n",
    "            \n",
    "            values_equal = fu_sorted.equals(combined_sorted)\n",
    "            result['values_equal'] = values_equal\n",
    "            \n",
    "            if values_equal:\n",
    "                print(f\"✅ fu_{n_plus_1} == dt_year_{n_plus_1} + fu_{n} (동일함)\")\n",
    "            else:\n",
    "                print(f\"❌ fu_{n_plus_1} != dt_year_{n_plus_1} + fu_{n} (값 차이 있음)\")\n",
    "                # 차이 확인\n",
    "                diff = fu_sorted.compare(combined_sorted)\n",
    "                print(f\"   차이 있는 행 수: {len(diff)}\")\n",
    "        else:\n",
    "            result['values_equal'] = None\n",
    "            if not cols_equal:\n",
    "                print(f\"❌ 칼럼이 다름\")\n",
    "                print(f\"   fu_{n_plus_1} 칼럼: {result['fu_n_plus_1_cols']}\")\n",
    "                print(f\"   합친 결과 칼럼: {result['combined_cols']}\")\n",
    "            if len(fu_n_plus_1) != len(combined):\n",
    "                print(f\"❌ 행 수가 다름: fu_{n_plus_1}={len(fu_n_plus_1)}, 합친 결과={len(combined)}\")\n",
    "        \n",
    "        comparison_results.append(result)\n",
    "        \n",
    "        # 메모리 최적화: fu_n을 fu_{n+1}로 덮어쓰기\n",
    "        fu_n = fu_n_plus_1.copy()\n",
    "        del fu_n_plus_1, dt_year_n_plus_1, combined\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ 파일을 찾을 수 없습니다: {e}\")\n",
    "        comparison_results.append({\n",
    "            'n': n,\n",
    "            'error': f'FileNotFoundError: {str(e)}'\n",
    "        })\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {e}\")\n",
    "        comparison_results.append({\n",
    "            'n': n,\n",
    "            'error': str(e)\n",
    "        })\n",
    "        break\n",
    "\n",
    "# 요약 출력\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"요약\")\n",
    "print(f\"{'='*80}\")\n",
    "success_count = sum(1 for r in comparison_results if r.get('values_equal') == True)\n",
    "total_count = len([r for r in comparison_results if 'error' not in r])\n",
    "print(f\"성공: {success_count}/{total_count}개\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12abbef5",
   "metadata": {},
   "source": [
    "## 3. std_pop3 파일 비교\n",
    "\n",
    "두 디렉토리에 있는 `std_pop3.sas7bdat` 파일을 비교하여 칼럼과 값의 차이점을 분석합니다.\n",
    "\n",
    "### 비교 대상\n",
    "- **파일 1**: `/home/hashjamm/project_data/disease_network/sas_files/hr_project/std_pop3.sas7bdat`\n",
    "- **파일 2**: `/home/hashjamm/project_data/disease_network/sas_files/std_pop3.sas7bdat`\n",
    "\n",
    "### 비교 항목\n",
    "1. **칼럼 차이**: 각 파일의 칼럼 구조 비교\n",
    "   - 파일1에만 있는 칼럼\n",
    "   - 파일2에만 있는 칼럼\n",
    "   - 공통 칼럼\n",
    "\n",
    "2. **값 차이**: 공통 칼럼의 데이터 값 비교\n",
    "   - 비교 조건: 행 수가 동일하고 `PERSON_ID` 종류가 동일한 경우에만 수행\n",
    "   - `PERSON_ID` 기준으로 정렬 후 값 비교\n",
    "   - 차이가 있는 칼럼 식별\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f559465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "std_pop3 파일 비교\n",
      "================================================================================\n",
      "\n",
      "파일 1: 2371213행, 41개 칼럼\n",
      "파일 2: 2371213행, 18개 칼럼\n",
      "\n",
      "📋 칼럼 비교:\n",
      "   파일1 칼럼: ['AGE_GROUP', 'CTRB_PT_TYPE_CD', 'DFAB_GRD_CD', 'DFAB_PTN_CD', 'DFAB_REG_YM', 'DMD_CT_TOT_AMT', 'DMD_DRG_NO', 'DMD_JBRDN_AMT', 'DMD_MRI_TOT_AMT', 'DMD_SBRDN_AMT', 'DMD_TRAMT', 'DSBJT_CD', 'DTH_CODE1', 'DTH_CODE2', 'DTH_YM', 'EDEC_ADD_RT', 'EDEC_CT_TOT_AMT', 'EDEC_JBRDN_AMT', 'EDEC_MRI_TOT_AMT', 'EDEC_SBRDN_AMT', 'EDEC_TRAMT', 'FORM_CD', 'FST_IN_PAT_DT', 'IN_PAT_CORS_TYPE', 'IPSN_TYPE_CD', 'KEY_SEQ', 'MAIN_SICK', 'MPRSC_GRANT_NO', 'OFFC_INJ_TYPE', 'PERSON_ID', 'RECN', 'RECU_FR_DT', 'SEX', 'SGG', 'SIDO', 'SUB_SICK', 'TOT_PRES_DD_CNT', 'VSCN', 'YKIHO_ID', 'abb_sick', 'recu_y']\n",
      "   파일2 칼럼: ['AGE_GROUP', 'CTRB_PT_TYPE_CD', 'DSBJT_CD', 'FORM_CD', 'FST_IN_PAT_DT', 'IN_PAT_CORS_TYPE', 'KEY_SEQ', 'MAIN_SICK', 'PERSON_ID', 'RECN', 'RECU_FR_DT', 'SEX', 'SGG', 'STND_Y', 'SUB_SICK', 'VSCN', 'abb_sick', 'recu_y']\n",
      "   공통 칼럼: ['AGE_GROUP', 'CTRB_PT_TYPE_CD', 'DSBJT_CD', 'FORM_CD', 'FST_IN_PAT_DT', 'IN_PAT_CORS_TYPE', 'KEY_SEQ', 'MAIN_SICK', 'PERSON_ID', 'RECN', 'RECU_FR_DT', 'SEX', 'SGG', 'SUB_SICK', 'VSCN', 'abb_sick', 'recu_y']\n",
      "   ⚠️ 파일1에만 있는 칼럼: ['DFAB_GRD_CD', 'DFAB_PTN_CD', 'DFAB_REG_YM', 'DMD_CT_TOT_AMT', 'DMD_DRG_NO', 'DMD_JBRDN_AMT', 'DMD_MRI_TOT_AMT', 'DMD_SBRDN_AMT', 'DMD_TRAMT', 'DTH_CODE1', 'DTH_CODE2', 'DTH_YM', 'EDEC_ADD_RT', 'EDEC_CT_TOT_AMT', 'EDEC_JBRDN_AMT', 'EDEC_MRI_TOT_AMT', 'EDEC_SBRDN_AMT', 'EDEC_TRAMT', 'IPSN_TYPE_CD', 'MPRSC_GRANT_NO', 'OFFC_INJ_TYPE', 'SIDO', 'TOT_PRES_DD_CNT', 'YKIHO_ID']\n",
      "   ⚠️ 파일2에만 있는 칼럼: ['STND_Y']\n",
      "\n",
      "📊 행 수 비교:\n",
      "   파일1: 2371213행\n",
      "   파일2: 2371213행\n",
      "   ✅ 행 수 동일\n",
      "\n",
      "📊 공통 칼럼 완전 일치 확인:\n",
      "   비교 대상 칼럼 (17개): ['AGE_GROUP', 'CTRB_PT_TYPE_CD', 'DSBJT_CD', 'FORM_CD', 'FST_IN_PAT_DT', 'IN_PAT_CORS_TYPE', 'KEY_SEQ', 'MAIN_SICK', 'PERSON_ID', 'RECN', 'RECU_FR_DT', 'SEX', 'SGG', 'SUB_SICK', 'VSCN', 'abb_sick', 'recu_y']\n",
      "\n",
      "   파일1: 총 2371213행, 중복 제거 후 2371213행\n",
      "   파일2: 총 2371213행, 중복 제거 후 2371213행\n",
      "\n",
      "   ⚠️ 공통 칼럼의 값이 다름\n",
      "\n",
      "   📈 차이 분석:\n",
      "      • 겹치는 행 (양쪽 모두 존재): 2367183개\n",
      "      • 파일1에만 있는 행: 4030개\n",
      "      • 파일2에만 있는 행: 4030개\n",
      "\n",
      "   파일1에만 있는 행 샘플 (처음 5개):\n",
      "     AGE_GROUP CTRB_PT_TYPE_CD DSBJT_CD FORM_CD FST_IN_PAT_DT  \\\n",
      "397          0               1       11      03           NaN   \n",
      "610          0               1       11      03           NaN   \n",
      "1047         0              10       11      02           NaN   \n",
      "1422         0              10       11      03           NaN   \n",
      "2524         0              10       11      03           NaN   \n",
      "\n",
      "     IN_PAT_CORS_TYPE       KEY_SEQ MAIN_SICK   PERSON_ID  RECN RECU_FR_DT  \\\n",
      "397                 -  200305723307      K561  45845808.0   1.0   20030610   \n",
      "610                 -  200310990527       J00  41662012.0   6.0   20030603   \n",
      "1047               31  200308873762      N390  43326473.0  13.0   20031102   \n",
      "1422                -  200301735312      J459  73745697.0   5.0   20031202   \n",
      "2524               32  200302938349      P599  21087313.0   2.0   20030218   \n",
      "\n",
      "     SEX    SGG SUB_SICK  VSCN abb_sick recu_y  \n",
      "397    1  44200      J00   1.0      K56   2003  \n",
      "610    1  42110      J20   6.0      J00   2003  \n",
      "1047   1  27230      A09  10.0      N39   2003  \n",
      "1422   1  41283      J12   5.0      J45   2003  \n",
      "2524   2  27140     E039   2.0      P59   2003  \n",
      "\n",
      "   파일2에만 있는 행 샘플 (처음 5개):\n",
      "     AGE_GROUP CTRB_PT_TYPE_CD DSBJT_CD FORM_CD FST_IN_PAT_DT  \\\n",
      "141          0               1       11      02           NaN   \n",
      "593          0               1       11      03           NaN   \n",
      "1043         0              10       11      02           NaN   \n",
      "1309         0              10       11      03           NaN   \n",
      "1578         0              10       11      03           NaN   \n",
      "\n",
      "     IN_PAT_CORS_TYPE       KEY_SEQ MAIN_SICK   PERSON_ID  RECN RECU_FR_DT  \\\n",
      "141                31  200304997272      K561  45845808.0  12.0   20030610   \n",
      "593                 -  200310539769       J00  41662012.0   6.0   20030603   \n",
      "1043               31  200301883401      J459  73745697.0  12.0   20031202   \n",
      "1309                -  200301063346      N390  43326473.0   3.0   20031102   \n",
      "1578                -  200303649849       P59  21087313.0   3.0   20030218   \n",
      "\n",
      "     SEX    SGG SUB_SICK  VSCN abb_sick recu_y  \n",
      "141    1  44200      NaN   5.0      K56   2003  \n",
      "593    1  42110      J20   6.0      J00   2003  \n",
      "1043   1  41283     G039   7.0      J45   2003  \n",
      "1309   1  27230     K529   3.0      N39   2003  \n",
      "1578   2  27140      NaN   3.0      P59   2003  \n"
     ]
    }
   ],
   "source": [
    "# std_pop3 파일 비교 (1번 항목과 동일한 로직)\n",
    "file1_path = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/std_pop3.sas7bdat'\n",
    "file2_path = '/home/hashjamm/project_data/disease_network/sas_files/std_pop3.sas7bdat'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"std_pop3 파일 비교\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # 파일 읽기\n",
    "    df1 = pd.read_sas(file1_path, encoding='utf-8')\n",
    "    df2 = pd.read_sas(file2_path, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\n파일 1: {len(df1)}행, {len(df1.columns)}개 칼럼\")\n",
    "    print(f\"파일 2: {len(df2)}행, {len(df2.columns)}개 칼럼\")\n",
    "    \n",
    "    # 칼럼 차이 확인\n",
    "    cols1 = set(df1.columns)\n",
    "    cols2 = set(df2.columns)\n",
    "    only_in_file1 = cols1 - cols2\n",
    "    only_in_file2 = cols2 - cols1\n",
    "    common_cols = cols1 & cols2\n",
    "    \n",
    "    print(f\"\\n📋 칼럼 비교:\")\n",
    "    print(f\"   파일1 칼럼: {sorted(list(cols1))}\")\n",
    "    print(f\"   파일2 칼럼: {sorted(list(cols2))}\")\n",
    "    print(f\"   공통 칼럼: {sorted(list(common_cols))}\")\n",
    "    \n",
    "    if len(only_in_file1) > 0:\n",
    "        print(f\"   ⚠️ 파일1에만 있는 칼럼: {sorted(list(only_in_file1))}\")\n",
    "    if len(only_in_file2) > 0:\n",
    "        print(f\"   ⚠️ 파일2에만 있는 칼럼: {sorted(list(only_in_file2))}\")\n",
    "    if len(only_in_file1) == 0 and len(only_in_file2) == 0:\n",
    "        print(f\"   ✅ 칼럼 동일\")\n",
    "    \n",
    "    # person_id 칼럼 확인\n",
    "    person_id_col = None\n",
    "    for col_name in ['PERSON_ID', 'person_id', 'Person_ID']:\n",
    "        if col_name in common_cols:\n",
    "            person_id_col = col_name\n",
    "            break\n",
    "    \n",
    "    # 행 수 비교\n",
    "    print(f\"\\n📊 행 수 비교:\")\n",
    "    print(f\"   파일1: {len(df1)}행\")\n",
    "    print(f\"   파일2: {len(df2)}행\")\n",
    "    rows_equal = len(df1) == len(df2)\n",
    "    if rows_equal:\n",
    "        print(f\"   ✅ 행 수 동일\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ 행 수 다름 (차이: {len(df1) - len(df2)}행)\")\n",
    "    \n",
    "    # 공통 칼럼(겹치는 칼럼)만 선택하여 완전 일치 확인\n",
    "    if len(common_cols) > 0:\n",
    "        common_cols_list = sorted(list(common_cols))\n",
    "        df1_common_only = df1[common_cols_list].copy()  # 공통 칼럼만\n",
    "        df2_common_only = df2[common_cols_list].copy()  # 공통 칼럼만\n",
    "        \n",
    "        print(f\"\\n📊 공통 칼럼 완전 일치 확인:\")\n",
    "        print(f\"   비교 대상 칼럼 ({len(common_cols_list)}개): {common_cols_list}\")\n",
    "        \n",
    "        # 중복 제거 (공통 칼럼 기준)\n",
    "        df1_unique = df1_common_only.drop_duplicates()\n",
    "        df2_unique = df2_common_only.drop_duplicates()\n",
    "        \n",
    "        print(f\"\\n   파일1: 총 {len(df1_common_only)}행, 중복 제거 후 {len(df1_unique)}행\")\n",
    "        print(f\"   파일2: 총 {len(df2_common_only)}행, 중복 제거 후 {len(df2_unique)}행\")\n",
    "        \n",
    "        # 정렬 후 완전 일치 확인\n",
    "        df1_sorted = df1_unique.sort_values(by=common_cols_list).reset_index(drop=True)\n",
    "        df2_sorted = df2_unique.sort_values(by=common_cols_list).reset_index(drop=True)\n",
    "        \n",
    "        # 완전 일치 여부 확인\n",
    "        is_identical = df1_sorted.equals(df2_sorted)\n",
    "        \n",
    "        if is_identical:\n",
    "            print(f\"\\n   ✅ 공통 칼럼의 값이 완전히 동일함\")\n",
    "        else:\n",
    "            print(f\"\\n   ⚠️ 공통 칼럼의 값이 다름\")\n",
    "            \n",
    "            # 차이 분석: 어느 쪽에만 있는 행 찾기\n",
    "            # merge를 사용하여 차이점 확인\n",
    "            merged = df1_unique.merge(df2_unique, on=common_cols_list, how='outer', indicator=True)\n",
    "            \n",
    "            # 양쪽에 모두 있는 행 (겹치는 행)\n",
    "            common_rows = merged[merged['_merge'] == 'both']\n",
    "            # 파일1에만 있는 행\n",
    "            only_in_file1 = merged[merged['_merge'] == 'left_only']\n",
    "            # 파일2에만 있는 행\n",
    "            only_in_file2 = merged[merged['_merge'] == 'right_only']\n",
    "            \n",
    "            print(f\"\\n   📈 차이 분석:\")\n",
    "            print(f\"      • 겹치는 행 (양쪽 모두 존재): {len(common_rows)}개\")\n",
    "            print(f\"      • 파일1에만 있는 행: {len(only_in_file1)}개\")\n",
    "            print(f\"      • 파일2에만 있는 행: {len(only_in_file2)}개\")\n",
    "            \n",
    "            # 샘플 출력\n",
    "            if len(only_in_file1) > 0:\n",
    "                print(f\"\\n   파일1에만 있는 행 샘플 (처음 5개):\")\n",
    "                print(only_in_file1[common_cols_list].head(5))\n",
    "            if len(only_in_file2) > 0:\n",
    "                print(f\"\\n   파일2에만 있는 행 샘플 (처음 5개):\")\n",
    "                print(only_in_file2[common_cols_list].head(5))\n",
    "    else:\n",
    "        print(f\"\\n📊 비교: ⚠️ 겹치는 칼럼이 없으므로 비교 불가\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ 파일을 찾을 수 없습니다: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 오류 발생: {e}\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8970a65",
   "metadata": {},
   "source": [
    "# [Issue Report] RR vs HR 프로젝트 간 데이터 정합성 분석 및 대응 전략\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 개요 (Issue Summary)\n",
    "\n",
    "**상황**: 기존 질병 네트워크(RR 기반, Code 1)와 후속 생존 분석(HR 기반, Code 2)의 데이터 정합성을 검증하던 중, 동일해야 할 `std_pop3` 테이블에서 일부 불일치 발견.\n",
    "\n",
    "### 불일치 내역\n",
    "\n",
    "- 전체 행 수(N)는 동일하나, 약 **0.17% (4,030건)**의 데이터에서 `KEY_SEQ`(청구일련번호) 등의 칼럼 값이 다름.\n",
    "- `PERSON_ID`, `RECU_FR_DT`(진료개시일) 등 핵심 식별자는 동일함.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 원인 분석 (Root Cause Analysis)\n",
    "\n",
    "### SAS PROC SORT의 비결정적(Non-deterministic) 특성\n",
    "\n",
    "- 두 코드 모두 `nodupkey by person_id abb_sick`을 사용하여 중복을 제거함.\n",
    "- 그러나 동일한 날짜(`RECU_FR_DT`)에 **2건 이상의 청구(다중 내원, 청구 분리 등)**가 존재하는 경우, 정렬 기준(`by`)에 `KEY_SEQ`가 포함되지 않아 무작위로 대표 행이 선택됨.\n",
    "\n",
    "### 칼럼 구성의 차이\n",
    "\n",
    "- Code 1은 일부 칼럼만 `KEEP`한 상태였고, Code 2는 전체 칼럼을 유지한 상태였음.\n",
    "- 이로 인해 메모리 상의 정렬/선택 과정에서 미세한 차이가 발생하여, 서로 다른 청구 건(예: 외래 vs 처방)이 선택됨.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 영향 평가 (Impact Assessment)\n",
    "\n",
    "### 통계적 유의성 (✅ 안전)\n",
    "\n",
    "- `matched` 코호트(모집단)가 양쪽 프로젝트에서 완벽히 동일함을 확인.\n",
    "- 불일치한 0.17%의 경우도 **\"해당 일자에 질병이 발생했다\"**는 사실(Event Occurrence) 자체는 변함이 없음.\n",
    "- 따라서 RR(Risk Ratio) 및 HR(Hazard Ratio)의 통계적 수치에는 영향이 없음.\n",
    "\n",
    "### 데이터 무결성 (⚠️ 주의)\n",
    "\n",
    "- RR 분석 결과에는 `KEY_SEQ` 정보가 누락되어 있거나 HR과 다른 건을 가리킬 수 있음.\n",
    "- 시각화 및 DB 구축 시 Node와 Edge를 구체적인 청구 건과 매핑할 때 정리가 필요함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb187a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "outcome_dt_year_10 파일 비교\n",
      "================================================================================\n",
      "\n",
      "파일 1: 1968727행, 2개 칼럼\n",
      "파일 2: 1968727행, 3개 칼럼\n",
      "\n",
      "📋 칼럼 비교:\n",
      "   파일1 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "   파일2 칼럼: ['KEY_SEQ', 'PERSON_ID', 'abb_sick']\n",
      "   공통 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "   ⚠️ 파일2에만 있는 칼럼: ['KEY_SEQ']\n",
      "\n",
      "📊 행 수 비교:\n",
      "   파일1: 1968727행\n",
      "   파일2: 1968727행\n",
      "   ✅ 행 수 동일\n",
      "\n",
      "📊 공통 칼럼 완전 일치 확인:\n",
      "   비교 대상 칼럼 (2개): ['PERSON_ID', 'abb_sick']\n",
      "\n",
      "   파일1: 총 1968727행, 중복 제거 후 1968727행\n",
      "   파일2: 총 1968727행, 중복 제거 후 1968727행\n",
      "\n",
      "   ✅ 공통 칼럼의 값이 완전히 동일함\n"
     ]
    }
   ],
   "source": [
    "# outcome_dt_year_10 파일 비교 (std_pop3 비교와 동일한 로직)\n",
    "file1_path = '/home/hashjamm/project_data/disease_network/sas_files/outcome_dt_year_10.sas7bdat'\n",
    "file2_path = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/outcome_dt_year_10.sas7bdat'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"outcome_dt_year_10 파일 비교\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # 파일 읽기\n",
    "    df1 = pd.read_sas(file1_path, encoding='utf-8')\n",
    "    df2 = pd.read_sas(file2_path, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\n파일 1: {len(df1)}행, {len(df1.columns)}개 칼럼\")\n",
    "    print(f\"파일 2: {len(df2)}행, {len(df2.columns)}개 칼럼\")\n",
    "    \n",
    "    # 칼럼 차이 확인\n",
    "    cols1 = set(df1.columns)\n",
    "    cols2 = set(df2.columns)\n",
    "    only_in_file1 = cols1 - cols2\n",
    "    only_in_file2 = cols2 - cols1\n",
    "    common_cols = cols1 & cols2\n",
    "    \n",
    "    print(f\"\\n📋 칼럼 비교:\")\n",
    "    print(f\"   파일1 칼럼: {sorted(list(cols1))}\")\n",
    "    print(f\"   파일2 칼럼: {sorted(list(cols2))}\")\n",
    "    print(f\"   공통 칼럼: {sorted(list(common_cols))}\")\n",
    "    \n",
    "    if len(only_in_file1) > 0:\n",
    "        print(f\"   ⚠️ 파일1에만 있는 칼럼: {sorted(list(only_in_file1))}\")\n",
    "    if len(only_in_file2) > 0:\n",
    "        print(f\"   ⚠️ 파일2에만 있는 칼럼: {sorted(list(only_in_file2))}\")\n",
    "    if len(only_in_file1) == 0 and len(only_in_file2) == 0:\n",
    "        print(f\"   ✅ 칼럼 동일\")\n",
    "    \n",
    "    # person_id 칼럼 확인\n",
    "    person_id_col = None\n",
    "    for col_name in ['PERSON_ID', 'person_id', 'Person_ID']:\n",
    "        if col_name in common_cols:\n",
    "            person_id_col = col_name\n",
    "            break\n",
    "    \n",
    "    # 행 수 비교\n",
    "    print(f\"\\n📊 행 수 비교:\")\n",
    "    print(f\"   파일1: {len(df1)}행\")\n",
    "    print(f\"   파일2: {len(df2)}행\")\n",
    "    rows_equal = len(df1) == len(df2)\n",
    "    if rows_equal:\n",
    "        print(f\"   ✅ 행 수 동일\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ 행 수 다름 (차이: {len(df1) - len(df2)}행)\")\n",
    "    \n",
    "    # 공통 칼럼(겹치는 칼럼)만 선택하여 완전 일치 확인\n",
    "    if len(common_cols) > 0:\n",
    "        common_cols_list = sorted(list(common_cols))\n",
    "        df1_common_only = df1[common_cols_list].copy()  # 공통 칼럼만\n",
    "        df2_common_only = df2[common_cols_list].copy()  # 공통 칼럼만\n",
    "        \n",
    "        print(f\"\\n📊 공통 칼럼 완전 일치 확인:\")\n",
    "        print(f\"   비교 대상 칼럼 ({len(common_cols_list)}개): {common_cols_list}\")\n",
    "        \n",
    "        # 중복 제거 (공통 칼럼 기준)\n",
    "        df1_unique = df1_common_only.drop_duplicates()\n",
    "        df2_unique = df2_common_only.drop_duplicates()\n",
    "        \n",
    "        print(f\"\\n   파일1: 총 {len(df1_common_only)}행, 중복 제거 후 {len(df1_unique)}행\")\n",
    "        print(f\"   파일2: 총 {len(df2_common_only)}행, 중복 제거 후 {len(df2_unique)}행\")\n",
    "        \n",
    "        # 정렬 후 완전 일치 확인\n",
    "        df1_sorted = df1_unique.sort_values(by=common_cols_list).reset_index(drop=True)\n",
    "        df2_sorted = df2_unique.sort_values(by=common_cols_list).reset_index(drop=True)\n",
    "        \n",
    "        # 완전 일치 여부 확인\n",
    "        is_identical = df1_sorted.equals(df2_sorted)\n",
    "        \n",
    "        if is_identical:\n",
    "            print(f\"\\n   ✅ 공통 칼럼의 값이 완전히 동일함\")\n",
    "        else:\n",
    "            print(f\"\\n   ⚠️ 공통 칼럼의 값이 다름\")\n",
    "            \n",
    "            # 차이 분석: 어느 쪽에만 있는 행 찾기\n",
    "            # merge를 사용하여 차이점 확인\n",
    "            merged = df1_unique.merge(df2_unique, on=common_cols_list, how='outer', indicator=True)\n",
    "            \n",
    "            # 양쪽에 모두 있는 행 (겹치는 행)\n",
    "            common_rows = merged[merged['_merge'] == 'both']\n",
    "            # 파일1에만 있는 행\n",
    "            only_in_file1 = merged[merged['_merge'] == 'left_only']\n",
    "            # 파일2에만 있는 행\n",
    "            only_in_file2 = merged[merged['_merge'] == 'right_only']\n",
    "            \n",
    "            print(f\"\\n   📈 차이 분석:\")\n",
    "            print(f\"      • 겹치는 행 (양쪽 모두 존재): {len(common_rows)}개\")\n",
    "            print(f\"      • 파일1에만 있는 행: {len(only_in_file1)}개\")\n",
    "            print(f\"      • 파일2에만 있는 행: {len(only_in_file2)}개\")\n",
    "            \n",
    "            # 샘플 출력\n",
    "            if len(only_in_file1) > 0:\n",
    "                print(f\"\\n   파일1에만 있는 행 샘플 (처음 5개):\")\n",
    "                print(only_in_file1[common_cols_list].head(5))\n",
    "            if len(only_in_file2) > 0:\n",
    "                print(f\"\\n   파일2에만 있는 행 샘플 (처음 5개):\")\n",
    "                print(only_in_file2[common_cols_list].head(5))\n",
    "    else:\n",
    "        print(f\"\\n📊 비교: ⚠️ 겹치는 칼럼이 없으므로 비교 불가\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ 파일을 찾을 수 없습니다: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 오류 발생: {e}\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a13d08f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "outcome_fu_10 파일 비교\n",
      "================================================================================\n",
      "\n",
      "파일 1: 23319782행, 2개 칼럼\n",
      "파일 2: 23319782행, 3개 칼럼\n",
      "\n",
      "📋 칼럼 비교:\n",
      "   파일1 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "   파일2 칼럼: ['KEY_SEQ', 'PERSON_ID', 'abb_sick']\n",
      "   공통 칼럼: ['PERSON_ID', 'abb_sick']\n",
      "   ⚠️ 파일2에만 있는 칼럼: ['KEY_SEQ']\n",
      "\n",
      "📊 행 수 비교:\n",
      "   파일1: 23319782행\n",
      "   파일2: 23319782행\n",
      "   ✅ 행 수 동일\n",
      "\n",
      "📊 공통 칼럼 완전 일치 확인:\n",
      "   비교 대상 칼럼 (2개): ['PERSON_ID', 'abb_sick']\n",
      "\n",
      "   파일1: 총 23319782행, 중복 제거 후 23319782행\n",
      "   파일2: 총 23319782행, 중복 제거 후 23319782행\n",
      "\n",
      "   ✅ 공통 칼럼의 값이 완전히 동일함\n"
     ]
    }
   ],
   "source": [
    "# outcome_fu_10 파일 비교 (std_pop3 비교와 동일한 로직)\n",
    "file1_path = '/home/hashjamm/project_data/disease_network/sas_files/outcome_fu_10.sas7bdat'\n",
    "file2_path = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/outcome_fu_10.sas7bdat'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"outcome_fu_10 파일 비교\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # 파일 읽기\n",
    "    df1 = pd.read_sas(file1_path, encoding='utf-8')\n",
    "    df2 = pd.read_sas(file2_path, encoding='utf-8')\n",
    "    \n",
    "    print(f\"\\n파일 1: {len(df1)}행, {len(df1.columns)}개 칼럼\")\n",
    "    print(f\"파일 2: {len(df2)}행, {len(df2.columns)}개 칼럼\")\n",
    "    \n",
    "    # 칼럼 차이 확인\n",
    "    cols1 = set(df1.columns)\n",
    "    cols2 = set(df2.columns)\n",
    "    only_in_file1 = cols1 - cols2\n",
    "    only_in_file2 = cols2 - cols1\n",
    "    common_cols = cols1 & cols2\n",
    "    \n",
    "    print(f\"\\n📋 칼럼 비교:\")\n",
    "    print(f\"   파일1 칼럼: {sorted(list(cols1))}\")\n",
    "    print(f\"   파일2 칼럼: {sorted(list(cols2))}\")\n",
    "    print(f\"   공통 칼럼: {sorted(list(common_cols))}\")\n",
    "    \n",
    "    if len(only_in_file1) > 0:\n",
    "        print(f\"   ⚠️ 파일1에만 있는 칼럼: {sorted(list(only_in_file1))}\")\n",
    "    if len(only_in_file2) > 0:\n",
    "        print(f\"   ⚠️ 파일2에만 있는 칼럼: {sorted(list(only_in_file2))}\")\n",
    "    if len(only_in_file1) == 0 and len(only_in_file2) == 0:\n",
    "        print(f\"   ✅ 칼럼 동일\")\n",
    "    \n",
    "    # person_id 칼럼 확인\n",
    "    person_id_col = None\n",
    "    for col_name in ['PERSON_ID', 'person_id', 'Person_ID']:\n",
    "        if col_name in common_cols:\n",
    "            person_id_col = col_name\n",
    "            break\n",
    "    \n",
    "    # 행 수 비교\n",
    "    print(f\"\\n📊 행 수 비교:\")\n",
    "    print(f\"   파일1: {len(df1)}행\")\n",
    "    print(f\"   파일2: {len(df2)}행\")\n",
    "    rows_equal = len(df1) == len(df2)\n",
    "    if rows_equal:\n",
    "        print(f\"   ✅ 행 수 동일\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ 행 수 다름 (차이: {len(df1) - len(df2)}행)\")\n",
    "    \n",
    "    # 공통 칼럼(겹치는 칼럼)만 선택하여 완전 일치 확인\n",
    "    if len(common_cols) > 0:\n",
    "        common_cols_list = sorted(list(common_cols))\n",
    "        df1_common_only = df1[common_cols_list].copy()  # 공통 칼럼만\n",
    "        df2_common_only = df2[common_cols_list].copy()  # 공통 칼럼만\n",
    "        \n",
    "        print(f\"\\n📊 공통 칼럼 완전 일치 확인:\")\n",
    "        print(f\"   비교 대상 칼럼 ({len(common_cols_list)}개): {common_cols_list}\")\n",
    "        \n",
    "        # 중복 제거 (공통 칼럼 기준)\n",
    "        df1_unique = df1_common_only.drop_duplicates()\n",
    "        df2_unique = df2_common_only.drop_duplicates()\n",
    "        \n",
    "        print(f\"\\n   파일1: 총 {len(df1_common_only)}행, 중복 제거 후 {len(df1_unique)}행\")\n",
    "        print(f\"   파일2: 총 {len(df2_common_only)}행, 중복 제거 후 {len(df2_unique)}행\")\n",
    "        \n",
    "        # 정렬 후 완전 일치 확인\n",
    "        df1_sorted = df1_unique.sort_values(by=common_cols_list).reset_index(drop=True)\n",
    "        df2_sorted = df2_unique.sort_values(by=common_cols_list).reset_index(drop=True)\n",
    "        \n",
    "        # 완전 일치 여부 확인\n",
    "        is_identical = df1_sorted.equals(df2_sorted)\n",
    "        \n",
    "        if is_identical:\n",
    "            print(f\"\\n   ✅ 공통 칼럼의 값이 완전히 동일함\")\n",
    "        else:\n",
    "            print(f\"\\n   ⚠️ 공통 칼럼의 값이 다름\")\n",
    "            \n",
    "            # 차이 분석: 어느 쪽에만 있는 행 찾기\n",
    "            # merge를 사용하여 차이점 확인\n",
    "            merged = df1_unique.merge(df2_unique, on=common_cols_list, how='outer', indicator=True)\n",
    "            \n",
    "            # 양쪽에 모두 있는 행 (겹치는 행)\n",
    "            common_rows = merged[merged['_merge'] == 'both']\n",
    "            # 파일1에만 있는 행\n",
    "            only_in_file1 = merged[merged['_merge'] == 'left_only']\n",
    "            # 파일2에만 있는 행\n",
    "            only_in_file2 = merged[merged['_merge'] == 'right_only']\n",
    "            \n",
    "            print(f\"\\n   📈 차이 분석:\")\n",
    "            print(f\"      • 겹치는 행 (양쪽 모두 존재): {len(common_rows)}개\")\n",
    "            print(f\"      • 파일1에만 있는 행: {len(only_in_file1)}개\")\n",
    "            print(f\"      • 파일2에만 있는 행: {len(only_in_file2)}개\")\n",
    "            \n",
    "            # 샘플 출력\n",
    "            if len(only_in_file1) > 0:\n",
    "                print(f\"\\n   파일1에만 있는 행 샘플 (처음 5개):\")\n",
    "                print(only_in_file1[common_cols_list].head(5))\n",
    "            if len(only_in_file2) > 0:\n",
    "                print(f\"\\n   파일2에만 있는 행 샘플 (처음 5개):\")\n",
    "                print(only_in_file2[common_cols_list].head(5))\n",
    "    else:\n",
    "        print(f\"\\n📊 비교: ⚠️ 겹치는 칼럼이 없으므로 비교 불가\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ 파일을 찾을 수 없습니다: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 오류 발생: {e}\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d14724",
   "metadata": {},
   "source": [
    "## 4. outcome_dt 및 outcome_fu 테이블 구조 분석 및 결론\n",
    "\n",
    "### 데이터 구조 확인 결과\n",
    "\n",
    "**RR 프로젝트의 핵심 테이블 구조**:\n",
    "- `outcome_dt_year_{n}`: `PERSON_ID`, `abb_sick` (2개 칼럼)\n",
    "- `outcome_fu_{n}`: `PERSON_ID`, `abb_sick` (2개 칼럼)\n",
    "\n",
    "**HR 프로젝트의 새로운 테이블 구조**:\n",
    "- `outcome_dt_year_{n}`: `KEY_SEQ`, `PERSON_ID`, `abb_sick` (3개 칼럼)\n",
    "- `outcome_fu_{n}`: `KEY_SEQ`, `PERSON_ID`, `abb_sick` (3개 칼럼)\n",
    "\n",
    "### 핵심 발견 사항\n",
    "\n",
    "1. **공통 칼럼의 완전 일치**: \n",
    "   - `PERSON_ID`와 `abb_sick` 기준으로 두 프로젝트의 데이터가 **100% 동일**함을 확인\n",
    "   - 행 수도 완전히 동일함\n",
    "\n",
    "2. **KEY_SEQ의 역할**:\n",
    "   - HR 버전에만 `KEY_SEQ` 칼럼이 추가되어 있음\n",
    "   - 이는 동일한 사람(`PERSON_ID`), 동일한 질병(`abb_sick`), 동일한 일자에 대해 여러 청구 건이 존재할 수 있음을 의미\n",
    "\n",
    "### SAS nodupkey 로직의 영향\n",
    "\n",
    "**SAS `PROC SORT nodupkey`의 비결정적 특성**:\n",
    "- 정렬 기준(`by`)에 포함된 칼럼 수에 따라 중복 제거 시 선택되는 행이 달라질 수 있음\n",
    "- RR 버전: `by person_id abb_sick` (2개 칼럼 기준)\n",
    "- HR 버전: `by person_id abb_sick key_seq` (3개 칼럼 기준, 또는 다른 칼럼 포함)\n",
    "\n",
    "**결과적으로**:\n",
    "- 같은 사람, 같은 질병, 같은 일자에 대해 **서로 다른 `KEY_SEQ`가 부여될 수 있음**\n",
    "- 그러나 `PERSON_ID`와 `abb_sick`의 조합은 완전히 동일하므로, **질병 발생 사건(Event) 자체는 변하지 않음**\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 최종 결론 및 대응 전략\n",
    "\n",
    "### ✅ RR 통계 수치의 무결성 보장\n",
    "\n",
    "- **RR 통계 계산 결과는 그대로 사용 가능**\n",
    "  - `PERSON_ID`와 `abb_sick` 기준으로 데이터가 완전히 동일함\n",
    "  - 질병 발생 사건의 빈도와 패턴이 동일하므로 Risk Ratio 계산에 영향 없음\n",
    "\n",
    "### ✅ edge_pids 데이터의 재사용 가능\n",
    "\n",
    "- **기존에 구해둔 `edge_pids` (Edge가 형성된 PID 리스트)는 그대로 사용 가능**\n",
    "  - Edge 형성은 `PERSON_ID`와 `abb_sick` 조합을 기반으로 하므로\n",
    "  - 네트워크 구조(Topology)는 변하지 않음\n",
    "\n",
    "### ⚠️ Edge의 KEY 값 매핑 전략\n",
    "\n",
    "- **Edge에 부여되는 `key_seq` 값은 새로운 HR 버전의 `outcome_dt` 및 `outcome_fu` 테이블을 따라야 함**\n",
    "  - RR 분석 시점에는 `KEY_SEQ` 정보가 없었거나 사용할 수 없었음\n",
    "  - 동일한 `(PERSON_ID, abb_sick, 날짜)` 조합에 대해 다른 `KEY_SEQ`가 선택될 수 있지만, 이는 네트워크 구조에는 영향 없음\n",
    "  - Edge의 `key_seq` 매핑은 HR 프로젝트의 `outcome_dt_year` 및 `outcome_fu` 테이블을 참조하여 수행\n",
    "\n",
    "### 구현 방향\n",
    "\n",
    "1. **RR 통계 및 edge_pids**: 기존 결과 그대로 유지 및 사용\n",
    "2. **Edge의 key_seq 매핑**: HR 프로젝트의 `outcome_dt_year` 및 `outcome_fu` 테이블을 Lookup 테이블로 사용하여 매핑\n",
    "3. **데이터 무결성**: 통계적 수치는 변하지 않으며, 단지 Edge에 대한 메타데이터(`key_seq`)만 새로 매핑하면 됨\n",
    "\n",
    "---\n",
    "\n",
    "**요약**: RR의 통계 수치와 네트워크 구조는 완전히 무결하게 유지되며, Edge의 `key_seq` 값만 HR 버전의 outcome 테이블을 기준으로 새로 매핑하면 됨.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749262f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 타입: <class 'dict'>\n",
      "\n",
      "딕셔너리 키 개수: 320428\n",
      "키 목록 (처음 10개): [('J03', 'J20'), ('J03', 'J06'), ('J03', 'J02'), ('J03', 'J00'), ('J03', 'K29'), ('J03', 'H10'), ('J03', 'L23'), ('J03', 'J30'), ('J03', 'J04'), ('J03', 'J01')]\n",
      "\n",
      "첫 번째 키: ('J03', 'J20')\n",
      "첫 번째 값 타입: <class 'numpy.ndarray'>\n",
      "첫 번째 값: [10115216 10370629 10377237 ... 95717664 95755730 95820119]\n",
      "\n",
      "전체 데이터 샘플 (처음 5개 항목):\n",
      "  ('J03', 'J20'): [10115216 10370629 10377237 ... 95717664 95755730 95820119]\n",
      "  ('J03', 'J06'): [10062980 10119030 10261227 ... 95613138 95707652 95822728]\n",
      "  ('J03', 'J02'): [10351340 10424308 10446492 ... 95708666 95788401 95819070]\n",
      "  ('J03', 'J00'): [10119030 10140151 10559362 ... 95630245 95698512 95761924]\n",
      "  ('J03', 'K29'): [10232145 10300910 10402504 ... 95766801 95788401 95806862]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = '/home/hashjamm/results/disease_network/edge_pids/edge_pids_10.pkl'\n",
    "\n",
    "# pickle 파일 로드\n",
    "with open(file_path, 'rb') as f:\n",
    "    edge_pids_10 = pickle.load(f)\n",
    "\n",
    "# 데이터 타입 확인\n",
    "print(f\"데이터 타입: {type(edge_pids_10)}\")\n",
    "\n",
    "# 데이터 구조 확인\n",
    "if isinstance(edge_pids_10, dict):\n",
    "    print(f\"\\n딕셔너리 키 개수: {len(edge_pids_10)}\")\n",
    "    print(f\"키 목록 (처음 10개): {list(edge_pids_10.keys())[:10]}\")\n",
    "    \n",
    "    # 첫 번째 키의 값 확인\n",
    "    if len(edge_pids_10) > 0:\n",
    "        first_key = list(edge_pids_10.keys())[0]\n",
    "        first_value = edge_pids_10[first_key]\n",
    "        print(f\"\\n첫 번째 키: {first_key}\")\n",
    "        print(f\"첫 번째 값 타입: {type(first_value)}\")\n",
    "        if isinstance(first_value, (list, set, tuple)):\n",
    "            print(f\"첫 번째 값 길이: {len(first_value)}\")\n",
    "            print(f\"첫 번째 값 샘플 (처음 10개): {list(first_value)[:10]}\")\n",
    "        elif isinstance(first_value, dict):\n",
    "            print(f\"첫 번째 값 키 개수: {len(first_value)}\")\n",
    "            print(f\"첫 번째 값 키 샘플: {list(first_value.keys())[:10]}\")\n",
    "        else:\n",
    "            print(f\"첫 번째 값: {first_value}\")\n",
    "    \n",
    "    # 전체 딕셔너리 샘플 (처음 5개 항목)\n",
    "    print(f\"\\n전체 데이터 샘플 (처음 5개 항목):\")\n",
    "    for i, (key, value) in enumerate(list(edge_pids_10.items())[:5]):\n",
    "        if isinstance(value, (list, set, tuple)):\n",
    "            print(f\"  {key}: {type(value).__name__} (길이: {len(value)}, 샘플: {list(value)[:5]})\")\n",
    "        elif isinstance(value, dict):\n",
    "            print(f\"  {key}: dict (키 개수: {len(value)}, 키 샘플: {list(value.keys())[:5]})\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "elif isinstance(edge_pids_10, (list, tuple)):\n",
    "    print(f\"\\n리스트/튜플 길이: {len(edge_pids_10)}\")\n",
    "    print(f\"처음 10개 항목:\")\n",
    "    for i, item in enumerate(edge_pids_10[:10]):\n",
    "        print(f\"  [{i}]: {item}\")\n",
    "\n",
    "elif isinstance(edge_pids_10, set):\n",
    "    print(f\"\\n셋 크기: {len(edge_pids_10)}\")\n",
    "    print(f\"처음 10개 항목:\")\n",
    "    for i, item in enumerate(list(edge_pids_10)[:10]):\n",
    "        print(f\"  [{i}]: {item}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n데이터: {edge_pids_10}\")\n",
    "    print(f\"데이터 크기/길이: {len(edge_pids_10) if hasattr(edge_pids_10, '__len__') else 'N/A'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55cee8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터프레임 shape: (991885, 15)\n",
      "\n",
      "칼럼 목록:\n",
      "['fu', 'cause_abb', 'outcome_abb', 'ct00', 'ct01', 'ct10', 'ct11', 'rr_values', 'rr_lower_cis', 'rr_upper_cis', 'log_rr_values', 'chisq_p_values', 'fisher_p_values', 'adjusted_chisq_p_values', 'adjusted_fisher_p_values']\n",
      "\n",
      "상위 5개 행:\n",
      "   fu cause_abb outcome_abb  ct00  ct01  ct10  ct11  rr_values  rr_lower_cis  \\\n",
      "0   1       A16         A15  7061     9  1280   134  74.444444     37.999309   \n",
      "1   1       K74         C22  4409     6   811    72  60.000000     26.171216   \n",
      "2   1       A15         A16  5072    13   897   120  46.153846     26.145321   \n",
      "3   1       G81         I69  1649     6   284    47  39.166667     16.884047   \n",
      "4   1       C22         K74  2085    10   344    75  37.500000     19.548704   \n",
      "\n",
      "   rr_upper_cis  log_rr_values  chisq_p_values  fisher_p_values  \\\n",
      "0    145.844107       4.310053   3.432395e-137     2.755249e-94   \n",
      "1    137.555703       4.094345    6.712756e-73     7.373630e-50   \n",
      "2     81.474520       3.831980   3.292205e-117     3.982694e-80   \n",
      "3     90.856638       3.667826    3.945854e-46     1.356317e-31   \n",
      "4     71.935713       3.624341    1.562308e-72     7.987700e-50   \n",
      "\n",
      "   adjusted_chisq_p_values  adjusted_fisher_p_values  \n",
      "0            2.692665e-134              1.385547e-91  \n",
      "1             1.865508e-70              1.405957e-47  \n",
      "2            1.859960e-114              1.584824e-77  \n",
      "3             5.305647e-44              1.277106e-29  \n",
      "4             4.281068e-70              1.520931e-47  \n",
      "\n",
      "fu가 10인 행 개수: 121652\n",
      "전체 행 대비 비율: 12.26%\n"
     ]
    }
   ],
   "source": [
    "# DBver_edge_stat.csv 파일 읽기 및 상위 5개 행 확인\n",
    "file_path = '/home/hashjamm/results/disease_network/db_migration/DBver_edge_stat.csv'\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 데이터프레임 정보 확인\n",
    "print(f\"데이터프레임 shape: {df.shape}\")\n",
    "print(f\"\\n칼럼 목록:\\n{df.columns.tolist()}\")\n",
    "print(f\"\\n상위 5개 행:\")\n",
    "print(df.head(5))\n",
    "\n",
    "# fu가 10인 경우의 행 개수 확인\n",
    "fu_10_count = len(df[df['fu'] == 10])\n",
    "print(f\"\\nfu가 10인 행 개수: {fu_10_count}\")\n",
    "print(f\"전체 행 대비 비율: {fu_10_count / len(df) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2200da",
   "metadata": {},
   "source": [
    "## Edge 데이터 구조 분석 및 정리\n",
    "\n",
    "### 1. R 코드 (hr_calculator_engine_v3.R)에서의 Edge 매핑 데이터\n",
    "\n",
    "R 코드에서 생성되는 Edge 매핑 데이터는 다음과 같이 구성됩니다:\n",
    "\n",
    "```r\n",
    "# Mapping 데이터\n",
    "key <- paste(cause_abb, outcome_abb, fu, sep = \"_\")\n",
    "pids <- clean_data[case == 1, .(person_id)]\n",
    "idx_key <- clean_data[case == 1, .(index_key_seq)]\n",
    "out_key <- clean_data[case == 1 & status == 1, .(key_seq)]\n",
    "```\n",
    "\n",
    "**각 변수의 의미:**\n",
    "\n",
    "1. **`edge_pids`** (`pids`):\n",
    "   - `case == 1`인 모든 사람들의 `person_id`\n",
    "   - 원인 질병을 가진 사람들 (case group) 전체\n",
    "   - 결과 질병 발생 여부와 무관하게 포함됨\n",
    "   - **실제로는 Edge가 아니라 원인 질병 노드에 속한 사람들의 정보**\n",
    "\n",
    "2. **`edge_index_key_seq`** (`idx_key`):\n",
    "   - `case == 1`인 모든 사람들의 `index_key_seq`\n",
    "   - 원인 질병 발생 시점의 키\n",
    "   - 결과 질병 발생 여부와 무관하게 포함됨\n",
    "   - **역시 Edge가 아니라 원인 질병 노드의 정보**\n",
    "\n",
    "3. **`edge_key_seq`** (`out_key`):\n",
    "   - `case == 1 & status == 1`인 사람들의 `key_seq`\n",
    "   - 원인 질병을 가지고 결과 질병도 발생한 사람들의 결과 질병 발생 시점 키\n",
    "   - **진짜 Edge를 대표하는 데이터**\n",
    "\n",
    "**결론:**\n",
    "- 결과 질병이 발생한 사람에 대한 데이터는 `out_key`만이 맞습니다.\n",
    "- `pids`와 `idx_key`는 원인 질병을 가진 전체 집단의 정보이며, Edge가 아닙니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Python 코드 (disease_network_funs.py)에서의 edge_pids 추출 방식\n",
    "\n",
    "Python 코드의 `process_disease_pair_unfiltered` 함수에서 `edge_pids`는 다음과 같이 추출됩니다:\n",
    "\n",
    "```python\n",
    "edge_pids = one_final_np[(one_final_np[:, 1] == 1) & (one_final_np[:, 2] == 1)][:, 0]\n",
    "```\n",
    "\n",
    "**추출 과정:**\n",
    "\n",
    "1. **데이터 준비:**\n",
    "   - `cause_np`: `[PERSON_ID, cause]` 형태 (matched 데이터)\n",
    "   - `outcome_np`: 결과 질병을 가진 사람들의 `PERSON_ID`\n",
    "\n",
    "2. **교집합 찾기:**\n",
    "   - 원인 질병 데이터와 결과 질병 데이터의 `PERSON_ID` 교집합 (`match_ids`)\n",
    "\n",
    "3. **병합 테이블 생성:**\n",
    "   - `one_final_np`: `[PERSON_ID, cause, outcome_status]` 형태\n",
    "   - `cause_np`의 모든 행을 복사 (LEFT JOIN과 유사)\n",
    "   - `match_ids`에 해당하는 경우에만 `outcome_status = 1`로 설정\n",
    "\n",
    "4. **edge_pids 추출:**\n",
    "   - `cause == 1` (원인 질병을 가진 사람) **AND**\n",
    "   - `outcome_status == 1` (결과 질병도 발생한 사람)\n",
    "   - 조건을 만족하는 `PERSON_ID`만 추출\n",
    "\n",
    "**결론:**\n",
    "- Python 코드의 `edge_pids`는 R 코드의 `out_key`와 동일한 로직입니다.\n",
    "- 원인 질병을 가진 사람 중에서 결과 질병도 발생한 사람들만 포함합니다.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. edge_pids_n의 시간 범위 의미 (중요!)\n",
    "\n",
    "**데이터 구조:**\n",
    "- `outcome_fu_{n+1}` == `outcome_dt_year_{n+1}` + `outcome_fu_n`\n",
    "- 예: `outcome_fu_2` == `outcome_dt_year_2` + `outcome_fu_1`\n",
    "- 즉, `outcome_dt_year_2`는 **fu=1에서 fu=2로 넘어가는 시점에 새로 발생한 질병들만** 포함합니다.\n",
    "\n",
    "**코드 흐름:**\n",
    "1. `ctable_2`는 `outcome_dt_year_2`를 사용하여 생성됩니다.\n",
    "2. `edge_pids_2`도 `outcome_dt_year_2`를 사용하여 생성됩니다.\n",
    "3. `updating_disease_pair` 함수는:\n",
    "   ```python\n",
    "   full_ctable_2 = fun_py.updating_disease_pair(full_ctable_1.to_numpy(), ctable_2.to_numpy())\n",
    "   ```\n",
    "   - `full_ctable_1` (fu=1까지의 누적 결과)에\n",
    "   - `ctable_2` (fu=1→fu=2 사이에 새로 발생한 엣지들)를 더하여\n",
    "   - `full_ctable_2` (fu=2까지의 누적 결과)를 만듭니다.\n",
    "\n",
    "**결론:**\n",
    "- **`edge_pids_2`는 fu=2 전체가 아니라, fu=1에서 fu=2로 넘어가는 시점에 새로 발생한 엣지들에 대한 pids만 포함합니다.**\n",
    "- 즉, `full_ctable_1`에 추가되는 새로운 엣지들에 대한 정보입니다.\n",
    "- 따라서 **fu=2 전체의 엣지를 보려면 `edge_pids_1`과 `edge_pids_2`를 합쳐야 합니다.**\n",
    "\n",
    "**일반화:**\n",
    "- `edge_pids_n` (n ≥ 2)는 fu=(n-1)에서 fu=n으로 넘어가는 시점에 새로 발생한 엣지들만 포함합니다.\n",
    "- fu=n 전체의 엣지를 보려면 `edge_pids_1`부터 `edge_pids_n`까지 모두 합쳐야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd5e6b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CSV 파일 조합 개수 vs edge_pids 조합 개수 비교\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "비교 중: n = 1\n",
      "================================================================================\n",
      "CSV 파일 읽기: /home/hashjamm/results/disease_network/final_results/DBver_cis_cut_final_result_1.csv\n",
      "CSV에 있는 조합 개수: 55,785개\n",
      "Pickle 파일 로드: /home/hashjamm/results/disease_network/edge_pids/edge_pids_1.pkl\n",
      "edge_pids_1의 전체 조합 개수: 367,932개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 55,785개\n",
      "  CSV에만 있는 조합: 0개\n",
      "  edge_pids에만 있는 조합: 312,147개\n",
      "\n",
      "================================================================================\n",
      "비교 중: n = 2\n",
      "================================================================================\n",
      "CSV 파일 읽기: /home/hashjamm/results/disease_network/final_results/DBver_cis_cut_final_result_2.csv\n",
      "CSV에 있는 조합 개수: 74,165개\n",
      "Pickle 파일 로드: /home/hashjamm/results/disease_network/edge_pids/edge_pids_2.pkl\n",
      "edge_pids_2의 전체 조합 개수: 341,239개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 73,801개\n",
      "  CSV에만 있는 조합: 364개\n",
      "  edge_pids에만 있는 조합: 267,438개\n",
      "\n",
      "================================================================================\n",
      "비교 중: n = 3\n",
      "================================================================================\n",
      "CSV 파일 읽기: /home/hashjamm/results/disease_network/final_results/DBver_cis_cut_final_result_3.csv\n",
      "CSV에 있는 조합 개수: 85,354개\n",
      "Pickle 파일 로드: /home/hashjamm/results/disease_network/edge_pids/edge_pids_3.pkl\n",
      "edge_pids_3의 전체 조합 개수: 332,967개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 83,854개\n",
      "  CSV에만 있는 조합: 1,500개\n",
      "  edge_pids에만 있는 조합: 249,113개\n",
      "\n",
      "================================================================================\n",
      "비교 중: n = 4\n",
      "================================================================================\n",
      "CSV 파일 읽기: /home/hashjamm/results/disease_network/final_results/DBver_cis_cut_final_result_4.csv\n",
      "CSV에 있는 조합 개수: 94,763개\n",
      "Pickle 파일 로드: /home/hashjamm/results/disease_network/edge_pids/edge_pids_4.pkl\n",
      "edge_pids_4의 전체 조합 개수: 335,706개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 91,889개\n",
      "  CSV에만 있는 조합: 2,874개\n",
      "  edge_pids에만 있는 조합: 243,817개\n",
      "\n",
      "================================================================================\n",
      "비교 중: n = 5\n",
      "================================================================================\n",
      "CSV 파일 읽기: /home/hashjamm/results/disease_network/final_results/DBver_cis_cut_final_result_5.csv\n",
      "CSV에 있는 조합 개수: 102,535개\n",
      "Pickle 파일 로드: /home/hashjamm/results/disease_network/edge_pids/edge_pids_5.pkl\n",
      "edge_pids_5의 전체 조합 개수: 331,601개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 97,960개\n",
      "  CSV에만 있는 조합: 4,575개\n",
      "  edge_pids에만 있는 조합: 233,641개\n",
      "\n",
      "================================================================================\n",
      "비교 중: n = 6\n",
      "================================================================================\n",
      "CSV 파일 읽기: /home/hashjamm/results/disease_network/final_results/DBver_cis_cut_final_result_6.csv\n",
      "CSV에 있는 조합 개수: 108,579개\n",
      "Pickle 파일 로드: /home/hashjamm/results/disease_network/edge_pids/edge_pids_6.pkl\n",
      "edge_pids_6의 전체 조합 개수: 328,097개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 102,264개\n",
      "  CSV에만 있는 조합: 6,315개\n",
      "  edge_pids에만 있는 조합: 225,833개\n",
      "\n",
      "================================================================================\n",
      "비교 중: n = 7\n",
      "================================================================================\n",
      "CSV 파일 읽기: /home/hashjamm/results/disease_network/final_results/DBver_cis_cut_final_result_7.csv\n",
      "CSV에 있는 조합 개수: 113,682개\n",
      "Pickle 파일 로드: /home/hashjamm/results/disease_network/edge_pids/edge_pids_7.pkl\n",
      "edge_pids_7의 전체 조합 개수: 323,245개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 105,454개\n",
      "  CSV에만 있는 조합: 8,228개\n",
      "  edge_pids에만 있는 조합: 217,791개\n",
      "\n",
      "================================================================================\n",
      "비교 중: n = 8\n",
      "================================================================================\n",
      "CSV 파일 읽기: /home/hashjamm/results/disease_network/final_results/DBver_cis_cut_final_result_8.csv\n",
      "CSV에 있는 조합 개수: 116,157개\n",
      "Pickle 파일 로드: /home/hashjamm/results/disease_network/edge_pids/edge_pids_8.pkl\n",
      "edge_pids_8의 전체 조합 개수: 330,057개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 106,952개\n",
      "  CSV에만 있는 조합: 9,205개\n",
      "  edge_pids에만 있는 조합: 223,105개\n",
      "\n",
      "================================================================================\n",
      "비교 중: n = 9\n",
      "================================================================================\n",
      "CSV 파일 읽기: /home/hashjamm/results/disease_network/final_results/DBver_cis_cut_final_result_9.csv\n",
      "CSV에 있는 조합 개수: 119,213개\n",
      "Pickle 파일 로드: /home/hashjamm/results/disease_network/edge_pids/edge_pids_9.pkl\n",
      "edge_pids_9의 전체 조합 개수: 326,263개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 108,358개\n",
      "  CSV에만 있는 조합: 10,855개\n",
      "  edge_pids에만 있는 조합: 217,905개\n",
      "\n",
      "================================================================================\n",
      "비교 중: n = 10\n",
      "================================================================================\n",
      "CSV 파일 읽기: /home/hashjamm/results/disease_network/final_results/DBver_cis_cut_final_result_10.csv\n",
      "CSV에 있는 조합 개수: 121,652개\n",
      "Pickle 파일 로드: /home/hashjamm/results/disease_network/edge_pids/edge_pids_10.pkl\n",
      "edge_pids_10의 전체 조합 개수: 320,428개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 108,840개\n",
      "  CSV에만 있는 조합: 12,812개\n",
      "  edge_pids에만 있는 조합: 211,588개\n",
      "\n",
      "================================================================================\n",
      "비교 완료!\n",
      "================================================================================\n",
      "\n",
      "비교 결과 요약:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "n     | CSV 조합       | edge_pids 조합    | 공통         | CSV만       | edge_pids만     \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1     |     55,785개 |       367,932개 |   55,785개 |        0개 |       312,147개\n",
      "2     |     74,165개 |       341,239개 |   73,801개 |      364개 |       267,438개\n",
      "3     |     85,354개 |       332,967개 |   83,854개 |    1,500개 |       249,113개\n",
      "4     |     94,763개 |       335,706개 |   91,889개 |    2,874개 |       243,817개\n",
      "5     |    102,535개 |       331,601개 |   97,960개 |    4,575개 |       233,641개\n",
      "6     |    108,579개 |       328,097개 |  102,264개 |    6,315개 |       225,833개\n",
      "7     |    113,682개 |       323,245개 |  105,454개 |    8,228개 |       217,791개\n",
      "8     |    116,157개 |       330,057개 |  106,952개 |    9,205개 |       223,105개\n",
      "9     |    119,213개 |       326,263개 |  108,358개 |   10,855개 |       217,905개\n",
      "10    |    121,652개 |       320,428개 |  108,840개 |   12,812개 |       211,588개\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 각 년도별 CSV 파일의 조합 개수와 edge_pids의 조합 개수 비교\n",
    "\n",
    "# 경로 설정\n",
    "final_results_dir = '/home/hashjamm/results/disease_network/final_results'\n",
    "edge_pids_dir = '/home/hashjamm/results/disease_network/edge_pids'\n",
    "\n",
    "# 처리할 n 값들 (1부터 10까지)\n",
    "n_values = list(range(1, 11))\n",
    "\n",
    "# 비교 결과 저장\n",
    "comparison_results = {}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CSV 파일 조합 개수 vs edge_pids 조합 개수 비교\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for n in n_values:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"비교 중: n = {n}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # CSV 파일 경로\n",
    "    csv_path = os.path.join(final_results_dir, f'DBver_cis_cut_final_result_{n}.csv')\n",
    "    \n",
    "    # pickle 파일 경로\n",
    "    pkl_path = os.path.join(edge_pids_dir, f'edge_pids_{n}.pkl')\n",
    "    \n",
    "    # 파일 존재 확인\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"⚠️ CSV 파일이 존재하지 않습니다: {csv_path}\")\n",
    "        continue\n",
    "    \n",
    "    if not os.path.exists(pkl_path):\n",
    "        print(f\"⚠️ Pickle 파일이 존재하지 않습니다: {pkl_path}\")\n",
    "        continue\n",
    "    \n",
    "    # CSV 파일 읽기\n",
    "    print(f\"CSV 파일 읽기: {csv_path}\")\n",
    "    df_final = pd.read_csv(csv_path)\n",
    "    \n",
    "    # cause_abb, outcome_abb 조합 추출\n",
    "    csv_combinations = set(zip(df_final['cause_abb'], df_final['outcome_abb']))\n",
    "    csv_count = len(csv_combinations)\n",
    "    print(f\"CSV에 있는 조합 개수: {csv_count:,}개\")\n",
    "    \n",
    "    # pickle 파일 로드\n",
    "    print(f\"Pickle 파일 로드: {pkl_path}\")\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        edge_pids_n = pickle.load(f)\n",
    "    \n",
    "    edge_pids_count = len(edge_pids_n)\n",
    "    print(f\"edge_pids_{n}의 전체 조합 개수: {edge_pids_count:,}개\")\n",
    "    \n",
    "    # 조합 비교\n",
    "    edge_pids_combos = set(edge_pids_n.keys())\n",
    "    \n",
    "    # 교집합, 차집합 계산\n",
    "    common_combos = csv_combinations & edge_pids_combos\n",
    "    only_in_csv = csv_combinations - edge_pids_combos\n",
    "    only_in_edge_pids = edge_pids_combos - csv_combinations\n",
    "    \n",
    "    print(f\"\\n비교 결과:\")\n",
    "    print(f\"  공통 조합: {len(common_combos):,}개\")\n",
    "    print(f\"  CSV에만 있는 조합: {len(only_in_csv):,}개\")\n",
    "    print(f\"  edge_pids에만 있는 조합: {len(only_in_edge_pids):,}개\")\n",
    "    \n",
    "    # 결과 저장\n",
    "    comparison_results[n] = {\n",
    "        'csv_count': csv_count,\n",
    "        'edge_pids_count': edge_pids_count,\n",
    "        'common_count': len(common_combos),\n",
    "        'only_in_csv': len(only_in_csv),\n",
    "        'only_in_edge_pids': len(only_in_edge_pids)\n",
    "    }\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"비교 완료!\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# 최종 요약 테이블\n",
    "print(\"비교 결과 요약:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'n':<5} | {'CSV 조합':<12} | {'edge_pids 조합':<15} | {'공통':<10} | {'CSV만':<10} | {'edge_pids만':<15}\")\n",
    "print(\"-\" * 100)\n",
    "for n in sorted(comparison_results.keys()):\n",
    "    result = comparison_results[n]\n",
    "    print(f\"{n:<5} | {result['csv_count']:>10,}개 | {result['edge_pids_count']:>13,}개 | \"\n",
    "          f\"{result['common_count']:>8,}개 | {result['only_in_csv']:>8,}개 | {result['only_in_edge_pids']:>13,}개\")\n",
    "print(\"-\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d4cea78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "가설 검증: edge_pids_n의 누적 특성 확인\n",
      "================================================================================\n",
      "\n",
      "가설: edge_pids_n (n ≥ 2)는 fu=(n-1)에서 fu=n으로 넘어가는 시점에\n",
      "      새로 발생한 엣지들만 포함하므로, fu=n 전체를 보려면\n",
      "      edge_pids_1부터 edge_pids_n까지 합쳐야 함\n",
      "\n",
      "검증 방법: 누적 edge_pids (1부터 n까지 합친 것)의 조합과 CSV 파일의 조합 비교\n",
      "\n",
      "\n",
      "================================================================================\n",
      "검증 중: n = 1\n",
      "================================================================================\n",
      "CSV 파일의 조합 개수: 55,785개\n",
      "누적 edge_pids (1부터 1까지 합친 결과)의 조합 개수: 367,932개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 55,785개\n",
      "  CSV에만 있는 조합: 0개\n",
      "  누적 edge_pids에만 있는 조합: 312,147개\n",
      "\n",
      "================================================================================\n",
      "검증 중: n = 2\n",
      "================================================================================\n",
      "CSV 파일의 조합 개수: 74,165개\n",
      "누적 edge_pids (1부터 2까지 합친 결과)의 조합 개수: 449,426개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 74,165개\n",
      "  CSV에만 있는 조합: 0개\n",
      "  누적 edge_pids에만 있는 조합: 375,261개\n",
      "\n",
      "================================================================================\n",
      "검증 중: n = 3\n",
      "================================================================================\n",
      "CSV 파일의 조합 개수: 85,354개\n",
      "누적 edge_pids (1부터 3까지 합친 결과)의 조합 개수: 501,236개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 85,354개\n",
      "  CSV에만 있는 조합: 0개\n",
      "  누적 edge_pids에만 있는 조합: 415,882개\n",
      "\n",
      "================================================================================\n",
      "검증 중: n = 4\n",
      "================================================================================\n",
      "CSV 파일의 조합 개수: 94,763개\n",
      "누적 edge_pids (1부터 4까지 합친 결과)의 조합 개수: 541,840개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 94,763개\n",
      "  CSV에만 있는 조합: 0개\n",
      "  누적 edge_pids에만 있는 조합: 447,077개\n",
      "\n",
      "================================================================================\n",
      "검증 중: n = 5\n",
      "================================================================================\n",
      "CSV 파일의 조합 개수: 102,535개\n",
      "누적 edge_pids (1부터 5까지 합친 결과)의 조합 개수: 573,918개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 102,535개\n",
      "  CSV에만 있는 조합: 0개\n",
      "  누적 edge_pids에만 있는 조합: 471,383개\n",
      "\n",
      "================================================================================\n",
      "검증 중: n = 6\n",
      "================================================================================\n",
      "CSV 파일의 조합 개수: 108,579개\n",
      "누적 edge_pids (1부터 6까지 합친 결과)의 조합 개수: 600,207개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 108,579개\n",
      "  CSV에만 있는 조합: 0개\n",
      "  누적 edge_pids에만 있는 조합: 491,628개\n",
      "\n",
      "================================================================================\n",
      "검증 중: n = 7\n",
      "================================================================================\n",
      "CSV 파일의 조합 개수: 113,682개\n",
      "누적 edge_pids (1부터 7까지 합친 결과)의 조합 개수: 622,410개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 113,682개\n",
      "  CSV에만 있는 조합: 0개\n",
      "  누적 edge_pids에만 있는 조합: 508,728개\n",
      "\n",
      "================================================================================\n",
      "검증 중: n = 8\n",
      "================================================================================\n",
      "CSV 파일의 조합 개수: 116,157개\n",
      "누적 edge_pids (1부터 8까지 합친 결과)의 조합 개수: 643,198개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 116,157개\n",
      "  CSV에만 있는 조합: 0개\n",
      "  누적 edge_pids에만 있는 조합: 527,041개\n",
      "\n",
      "================================================================================\n",
      "검증 중: n = 9\n",
      "================================================================================\n",
      "CSV 파일의 조합 개수: 119,213개\n",
      "누적 edge_pids (1부터 9까지 합친 결과)의 조합 개수: 661,589개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 119,213개\n",
      "  CSV에만 있는 조합: 0개\n",
      "  누적 edge_pids에만 있는 조합: 542,376개\n",
      "\n",
      "================================================================================\n",
      "검증 중: n = 10\n",
      "================================================================================\n",
      "CSV 파일의 조합 개수: 121,652개\n",
      "누적 edge_pids (1부터 10까지 합친 결과)의 조합 개수: 677,461개\n",
      "\n",
      "비교 결과:\n",
      "  공통 조합: 121,652개\n",
      "  CSV에만 있는 조합: 0개\n",
      "  누적 edge_pids에만 있는 조합: 555,809개\n",
      "\n",
      "================================================================================\n",
      "검증 완료!\n",
      "================================================================================\n",
      "\n",
      "검증 결과 요약:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "n     | CSV 조합       | 누적 조합           | 공통         | CSV만       | 누적만             | 결과        \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1     |     55,785개 |       367,932개 |   55,785개 |        0개 |       312,147개 | ✅ 통과\n",
      "2     |     74,165개 |       449,426개 |   74,165개 |        0개 |       375,261개 | ✅ 통과\n",
      "3     |     85,354개 |       501,236개 |   85,354개 |        0개 |       415,882개 | ✅ 통과\n",
      "4     |     94,763개 |       541,840개 |   94,763개 |        0개 |       447,077개 | ✅ 통과\n",
      "5     |    102,535개 |       573,918개 |  102,535개 |        0개 |       471,383개 | ✅ 통과\n",
      "6     |    108,579개 |       600,207개 |  108,579개 |        0개 |       491,628개 | ✅ 통과\n",
      "7     |    113,682개 |       622,410개 |  113,682개 |        0개 |       508,728개 | ✅ 통과\n",
      "8     |    116,157개 |       643,198개 |  116,157개 |        0개 |       527,041개 | ✅ 통과\n",
      "9     |    119,213개 |       661,589개 |  119,213개 |        0개 |       542,376개 | ✅ 통과\n",
      "10    |    121,652개 |       677,461개 |  121,652개 |        0개 |       555,809개 | ✅ 통과\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "✅ 가설이 검증되었습니다!\n",
      "   CSV 파일의 모든 조합이 누적 edge_pids에 포함되어 있습니다.\n",
      "   즉, edge_pids_n (n ≥ 2)는 fu=(n-1)에서 fu=n으로 넘어가는 시점에\n",
      "   새로 발생한 엣지들만 포함하며, fu=n 전체를 보려면\n",
      "   edge_pids_1부터 edge_pids_n까지 합쳐야 합니다.\n"
     ]
    }
   ],
   "source": [
    "# 가설 검증: edge_pids_n이 누적 데이터인지 확인\n",
    "# 가설: edge_pids_n (n ≥ 2)는 fu=(n-1)에서 fu=n으로 넘어가는 시점에 새로 발생한 엣지들만 포함\n",
    "# 따라서 fu=n 전체의 엣지를 보려면 edge_pids_1부터 edge_pids_n까지 합쳐야 함\n",
    "# 검증 방법: 누적 edge_pids (1부터 n까지 합친 것)의 조합과 CSV 파일의 조합을 비교\n",
    "\n",
    "# 경로 설정\n",
    "final_results_dir = '/home/hashjamm/results/disease_network/final_results'\n",
    "edge_pids_dir = '/home/hashjamm/results/disease_network/edge_pids'\n",
    "\n",
    "# 처리할 n 값들 (1부터 10까지)\n",
    "n_values = list(range(1, 11))\n",
    "\n",
    "# 검증 결과 저장\n",
    "verification_results = {}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"가설 검증: edge_pids_n의 누적 특성 확인\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n가설: edge_pids_n (n ≥ 2)는 fu=(n-1)에서 fu=n으로 넘어가는 시점에\")\n",
    "print(\"      새로 발생한 엣지들만 포함하므로, fu=n 전체를 보려면\")\n",
    "print(\"      edge_pids_1부터 edge_pids_n까지 합쳐야 함\")\n",
    "print(\"\\n검증 방법: 누적 edge_pids (1부터 n까지 합친 것)의 조합과 CSV 파일의 조합 비교\\n\")\n",
    "\n",
    "for n in n_values:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"검증 중: n = {n}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 1. CSV 파일에서 조합 읽기\n",
    "    csv_path = os.path.join(final_results_dir, f'DBver_cis_cut_final_result_{n}.csv')\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"⚠️ CSV 파일이 존재하지 않습니다: {csv_path}\")\n",
    "        continue\n",
    "    \n",
    "    df_final = pd.read_csv(csv_path)\n",
    "    csv_combinations = set(zip(df_final['cause_abb'], df_final['outcome_abb']))\n",
    "    csv_count = len(csv_combinations)\n",
    "    print(f\"CSV 파일의 조합 개수: {csv_count:,}개\")\n",
    "    \n",
    "    # 2. edge_pids_1부터 edge_pids_n까지 로드하여 누적\n",
    "    cumulative_edge_pids_combos = set()\n",
    "    \n",
    "    for i in range(1, n + 1):\n",
    "        pkl_path = os.path.join(edge_pids_dir, f'edge_pids_{i}.pkl')\n",
    "        \n",
    "        if not os.path.exists(pkl_path):\n",
    "            print(f\"⚠️ 파일이 존재하지 않습니다: {pkl_path}\")\n",
    "            continue\n",
    "        \n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            edge_pids_i = pickle.load(f)\n",
    "        \n",
    "        # edge_pids_i의 모든 조합을 누적\n",
    "        cumulative_edge_pids_combos.update(edge_pids_i.keys())\n",
    "    \n",
    "    cumulative_count = len(cumulative_edge_pids_combos)\n",
    "    print(f\"누적 edge_pids (1부터 {n}까지 합친 결과)의 조합 개수: {cumulative_count:,}개\")\n",
    "    \n",
    "    # 3. 조합 비교\n",
    "    common_combos = csv_combinations & cumulative_edge_pids_combos\n",
    "    only_in_csv = csv_combinations - cumulative_edge_pids_combos\n",
    "    only_in_cumulative = cumulative_edge_pids_combos - csv_combinations\n",
    "    \n",
    "    print(f\"\\n비교 결과:\")\n",
    "    print(f\"  공통 조합: {len(common_combos):,}개\")\n",
    "    print(f\"  CSV에만 있는 조합: {len(only_in_csv):,}개\")\n",
    "    print(f\"  누적 edge_pids에만 있는 조합: {len(only_in_cumulative):,}개\")\n",
    "    \n",
    "    # 검증 결과 저장\n",
    "    verification_results[n] = {\n",
    "        'csv_count': csv_count,\n",
    "        'cumulative_count': cumulative_count,\n",
    "        'common_count': len(common_combos),\n",
    "        'only_in_csv': len(only_in_csv),\n",
    "        'only_in_cumulative': len(only_in_cumulative),\n",
    "        'is_valid': len(only_in_csv) == 0  # CSV의 모든 조합이 누적에 있어야 함\n",
    "    }\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"검증 완료!\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# 최종 요약\n",
    "print(\"검증 결과 요약:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'n':<5} | {'CSV 조합':<12} | {'누적 조합':<15} | {'공통':<10} | {'CSV만':<10} | {'누적만':<15} | {'결과':<10}\")\n",
    "print(\"-\" * 100)\n",
    "for n in sorted(verification_results.keys()):\n",
    "    result = verification_results[n]\n",
    "    status = \"✅ 통과\" if result['is_valid'] else \"❌ 실패\"\n",
    "    print(f\"{n:<5} | {result['csv_count']:>10,}개 | {result['cumulative_count']:>13,}개 | \"\n",
    "          f\"{result['common_count']:>8,}개 | {result['only_in_csv']:>8,}개 | \"\n",
    "          f\"{result['only_in_cumulative']:>13,}개 | {status}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# 전체 통과 여부\n",
    "all_valid = all(r['is_valid'] for r in verification_results.values())\n",
    "if all_valid:\n",
    "    print(\"\\n✅ 가설이 검증되었습니다!\")\n",
    "    print(\"   CSV 파일의 모든 조합이 누적 edge_pids에 포함되어 있습니다.\")\n",
    "    print(\"   즉, edge_pids_n (n ≥ 2)는 fu=(n-1)에서 fu=n으로 넘어가는 시점에\")\n",
    "    print(\"   새로 발생한 엣지들만 포함하며, fu=n 전체를 보려면\")\n",
    "    print(\"   edge_pids_1부터 edge_pids_n까지 합쳐야 합니다.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ 일부 검증 실패가 있습니다.\")\n",
    "    print(\"   CSV에만 있는 조합이 있다는 것은 누적 edge_pids에 해당 조합이 없다는 의미입니다.\")\n",
    "    print(\"   위의 상세 정보를 확인하세요.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b97a6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow 엔진 실패: A type extension with name pandas.period already defined\n",
      "fastparquet 엔진으로 재시도...\n",
      "================================================================================\n",
      "edge_pids_mapping_10.parquet 파일 정보\n",
      "================================================================================\n",
      "\n",
      "전체 행 수: 12,177개\n",
      "칼럼 수: 2개\n",
      "\n",
      "칼럼 목록:\n",
      "  1. key\n",
      "  2. values\n",
      "\n",
      "================================================================================\n",
      "상위 10개 행:\n",
      "================================================================================\n",
      "          key                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 values\n",
      "0  A08_R19_10  [13821648.0, 15352200.0, 15352687.0, 34167025.0, 83527916.0, 85762291.0, 30144696.0, 30146431.0, 31934888.0, 23960986.0, 20604456.0, 36895938.0, 37785347.0, 37994329.0, 15953155.0, 44091608.0, 42306091.0, 93690845.0, 80609459.0, 83218147.0, 76513918.0, 66927812.0, 65534910.0, 88102220.0, 88657793.0, 88840421.0, 89410641.0, 70981004.0, 72585522.0, 73183736.0, 77287741.0, 79994094.0, 58413185.0, 58759556.0, 53950711.0, 54202169.0, 46679078.0, 47351123.0, 47627311.0, 48786647.0, 91553754.0, 11310199.0, 11477598.0, 11992004.0, 12572706.0, 12825451.0, 13230085.0, 13272858.0, 13488430.0, 13802581.0, 13904150.0, 13949283.0, 14143116.0, 14566519.0, 14663910.0, 15061248.0, 15569979.0, 15652754.0, 15772762.0, 16057722.0, 16122237.0, 16188336.0, 16285752.0, 16473454.0, 16657981.0, 17383111.0, 17658147.0, 17711688.0, 17838568.0, 17949315.0, 18014095.0, 19404476.0, 20705848.0, 20793935.0, 20906994.0, 21218706.0, 21647354.0, 21900643.0, 22022424.0, 22290133.0, 22859081.0, 22957609.0, 23289758.0, 23307174.0, 23351075.0, 23523185.0, 23780550.0, 23936642.0, 24245850.0, 24376806.0, 25471014.0, 25581305.0, 26142587.0, 27196773.0, 27241495.0, 27495481.0, 27529111.0, 27912686.0, 28063861.0, 28075557.0, ...]\n",
      "1  A08_R20_10  [36973544.0, 62203211.0, 53037668.0, 35275581.0, 27784915.0, 68355169.0, 75727735.0, 65056055.0, 92482224.0, 88657793.0, 11310199.0, 11477598.0, 11992004.0, 12572706.0, 12825451.0, 13230085.0, 13272858.0, 13488430.0, 13802581.0, 13904150.0, 13949283.0, 14143116.0, 14566519.0, 14663910.0, 15061248.0, 15569979.0, 15652754.0, 15772762.0, 16057722.0, 16122237.0, 16188336.0, 16285752.0, 16473454.0, 16657981.0, 17383111.0, 17658147.0, 17711688.0, 17838568.0, 17949315.0, 18014095.0, 19404476.0, 20604456.0, 20705848.0, 20793935.0, 20906994.0, 21218706.0, 21647354.0, 21900643.0, 22022424.0, 22290133.0, 22859081.0, 22957609.0, 23289758.0, 23307174.0, 23351075.0, 23523185.0, 23780550.0, 23936642.0, 24245850.0, 24376806.0, 25471014.0, 25581305.0, 26142587.0, 27196773.0, 27241495.0, 27495481.0, 27529111.0, 27912686.0, 28063861.0, 28075557.0, 28397130.0, 29041970.0, 29210467.0, 29217676.0, 29251501.0, 29272286.0, 29311113.0, 29381888.0, 29516613.0, 30129958.0, 30173861.0, 30237844.0, 30530242.0, 30785172.0, 31132508.0, 31244375.0, 31902197.0, 32943861.0, 33053414.0, 33218033.0, 33234788.0, 33771666.0, 34045140.0, 34350573.0, 34692520.0, 34697512.0, 35264732.0, 35308975.0, 35324490.0, 35550573.0, ...]\n",
      "2  A08_R21_10  [23025388.0, 23412838.0, 24082723.0, 25783902.0, 61891687.0, 61997905.0, 62141825.0, 63155506.0, 51078282.0, 51478367.0, 26879735.0, 27120653.0, 27133897.0, 28533713.0, 54005035.0, 54170236.0, 94103652.0, 95598356.0, 19381282.0, 35624664.0, 39985088.0, 40337472.0, 40559765.0, 74486213.0, 76057940.0, 13949283.0, 76757789.0, 78663724.0, 79994094.0, 57877507.0, 58678091.0, 58925449.0, 83038422.0, 83298671.0, 63528184.0, 66246840.0, 86743278.0, 87713260.0, 89233042.0, 36589042.0, 37994329.0, 38744726.0, 72092553.0, 67347729.0, 68339897.0, 68388066.0, 68925755.0, 69488326.0, 89798335.0, 90341647.0, 90624053.0, 92179463.0, 92799700.0, 47552558.0, 47695710.0, 48174305.0, 43230092.0, 45498398.0, 11310199.0, 11477598.0, 11992004.0, 12572706.0, 12825451.0, 13230085.0, 13272858.0, 13488430.0, 13802581.0, 13904150.0, 14143116.0, 14566519.0, 14663910.0, 15061248.0, 15569979.0, 15652754.0, 15772762.0, 16057722.0, 16122237.0, 16188336.0, 16473454.0, 16657981.0, 17383111.0, 17658147.0, 17711688.0, 17838568.0, 17949315.0, 18014095.0, 19404476.0, 20604456.0, 20705848.0, 20793935.0, 20906994.0, 21218706.0, 21647354.0, 21900643.0, 22022424.0, 22290133.0, 22957609.0, 23289758.0, 23307174.0, 23351075.0, ...]\n",
      "3  A08_R22_10  [26241980.0, 26308251.0, 28397130.0, 16288877.0, 18691673.0, 36353313.0, 37522890.0, 37640986.0, 37641035.0, 38597371.0, 19381282.0, 29732179.0, 15386584.0, 60425368.0, 57868334.0, 57877507.0, 58087441.0, 58594339.0, 59341986.0, 59650569.0, 94751601.0, 95176530.0, 95737283.0, 43626558.0, 46024780.0, 89897741.0, 90728908.0, 92476536.0, 63953614.0, 53868624.0, 68925755.0, 69319088.0, 71906380.0, 33398221.0, 81064911.0, 82028799.0, 82559920.0, 83009144.0, 77289866.0, 78663724.0, 79067639.0, 74104071.0, 83863496.0, 87362633.0, 87713260.0, 88618736.0, 88838934.0, 25140216.0, 11310199.0, 11477598.0, 11992004.0, 12572706.0, 12825451.0, 13230085.0, 13272858.0, 13488430.0, 13802581.0, 13904150.0, 13949283.0, 14143116.0, 14566519.0, 14663910.0, 15061248.0, 15569979.0, 15652754.0, 15772762.0, 16057722.0, 16122237.0, 16188336.0, 16285752.0, 16473454.0, 16657981.0, 17383111.0, 17658147.0, 17711688.0, 17838568.0, 17949315.0, 18014095.0, 19404476.0, 20604456.0, 20705848.0, 20793935.0, 20906994.0, 21218706.0, 21647354.0, 21900643.0, 22022424.0, 22290133.0, 22859081.0, 22957609.0, 23289758.0, 23307174.0, 23351075.0, 23523185.0, 23780550.0, 23936642.0, 24245850.0, 24376806.0, 25471014.0, 25581305.0, ...]\n",
      "4  A08_R23_10  [21687392.0, 23319136.0, 94282610.0, 82188298.0, 83761831.0, 55290943.0, 65839504.0, 11811859.0, 87434941.0, 40175363.0, 41605756.0, 11310199.0, 11477598.0, 11992004.0, 12572706.0, 12825451.0, 13230085.0, 13272858.0, 13488430.0, 13802581.0, 13904150.0, 13949283.0, 14143116.0, 14566519.0, 14663910.0, 15061248.0, 15569979.0, 15652754.0, 15772762.0, 16057722.0, 16122237.0, 16188336.0, 16285752.0, 16473454.0, 16657981.0, 17383111.0, 17658147.0, 17711688.0, 17838568.0, 17949315.0, 18014095.0, 19404476.0, 20604456.0, 20705848.0, 20793935.0, 20906994.0, 21218706.0, 21647354.0, 21900643.0, 22022424.0, 22290133.0, 22859081.0, 22957609.0, 23289758.0, 23307174.0, 23351075.0, 23523185.0, 23780550.0, 23936642.0, 24245850.0, 24376806.0, 25471014.0, 25581305.0, 26142587.0, 27196773.0, 27241495.0, 27495481.0, 27529111.0, 27912686.0, 28063861.0, 28075557.0, 28397130.0, 29041970.0, 29210467.0, 29217676.0, 29251501.0, 29272286.0, 29311113.0, 29381888.0, 29516613.0, 30129958.0, 30173861.0, 30237844.0, 30530242.0, 30785172.0, 31132508.0, 31244375.0, 31902197.0, 32943861.0, 33053414.0, 33218033.0, 33234788.0, 33771666.0, 34045140.0, 34350573.0, 34692520.0, 34697512.0, 35264732.0, 35308975.0, 35324490.0, ...]\n",
      "5  A08_R25_10  [80336837.0, 82821886.0, 85274810.0, 77895892.0, 87651493.0, 72700724.0, 73253575.0, 39162855.0, 55072417.0, 26101671.0, 26879735.0, 28450917.0, 16644134.0, 65831175.0, 41721320.0, 11310199.0, 11477598.0, 11992004.0, 12572706.0, 12825451.0, 13230085.0, 13272858.0, 13488430.0, 13802581.0, 13904150.0, 13949283.0, 14143116.0, 14566519.0, 14663910.0, 15061248.0, 15569979.0, 15652754.0, 15772762.0, 16057722.0, 16122237.0, 16188336.0, 16285752.0, 16473454.0, 16657981.0, 17383111.0, 17658147.0, 17711688.0, 17838568.0, 17949315.0, 18014095.0, 19404476.0, 20604456.0, 20705848.0, 20793935.0, 20906994.0, 21218706.0, 21647354.0, 21900643.0, 22022424.0, 22290133.0, 22859081.0, 22957609.0, 23289758.0, 23307174.0, 23351075.0, 23523185.0, 23780550.0, 23936642.0, 24245850.0, 24376806.0, 25471014.0, 25581305.0, 26142587.0, 27196773.0, 27241495.0, 27495481.0, 27529111.0, 27912686.0, 28063861.0, 28075557.0, 28397130.0, 29041970.0, 29210467.0, 29217676.0, 29251501.0, 29272286.0, 29311113.0, 29381888.0, 29516613.0, 30129958.0, 30173861.0, 30237844.0, 30530242.0, 30785172.0, 31132508.0, 31244375.0, 31902197.0, 32943861.0, 33053414.0, 33218033.0, 33234788.0, 33771666.0, 34045140.0, 34350573.0, 34692520.0, ...]\n",
      "6  A08_R26_10  [15750143.0, 11911330.0, 14003070.0, 15569979.0, 46006422.0, 46338515.0, 83623241.0, 80424481.0, 83212398.0, 57062280.0, 57322660.0, 58720729.0, 58725586.0, 59994231.0, 48348354.0, 48933955.0, 92947906.0, 70461882.0, 70877449.0, 65272368.0, 79771949.0, 67096243.0, 69298210.0, 86760335.0, 73821599.0, 76479045.0, 11310199.0, 11477598.0, 11992004.0, 12572706.0, 12825451.0, 13230085.0, 13272858.0, 13488430.0, 13802581.0, 13904150.0, 13949283.0, 14143116.0, 14566519.0, 14663910.0, 15061248.0, 15652754.0, 15772762.0, 16057722.0, 16122237.0, 16188336.0, 16285752.0, 16473454.0, 16657981.0, 17383111.0, 17658147.0, 17711688.0, 17838568.0, 17949315.0, 18014095.0, 19404476.0, 20604456.0, 20705848.0, 20793935.0, 20906994.0, 21218706.0, 21647354.0, 21900643.0, 22022424.0, 22290133.0, 22859081.0, 22957609.0, 23289758.0, 23307174.0, 23351075.0, 23523185.0, 23780550.0, 23936642.0, 24245850.0, 24376806.0, 25471014.0, 25581305.0, 26142587.0, 27196773.0, 27241495.0, 27495481.0, 27529111.0, 27912686.0, 28063861.0, 28075557.0, 28397130.0, 29041970.0, 29210467.0, 29217676.0, 29251501.0, 29272286.0, 29311113.0, 29381888.0, 29516613.0, 30129958.0, 30173861.0, 30237844.0, 30530242.0, 30785172.0, 31132508.0, ...]\n",
      "7  A08_R29_10  [16203387.0, 18748510.0, 20566827.0, 24287583.0, 40541355.0, 40704323.0, 30294827.0, 32080314.0, 71945816.0, 93233145.0, 95704818.0, 57176493.0, 58718650.0, 59626408.0, 59748064.0, 81276928.0, 36025021.0, 36119775.0, 39178614.0, 55085319.0, 55326000.0, 62273896.0, 89798335.0, 85234598.0, 46713047.0, 48356923.0, 48969468.0, 46240731.0, 11310199.0, 11477598.0, 11992004.0, 12572706.0, 12825451.0, 13230085.0, 13272858.0, 13488430.0, 13802581.0, 13904150.0, 13949283.0, 14143116.0, 14566519.0, 14663910.0, 15061248.0, 15569979.0, 15652754.0, 15772762.0, 16057722.0, 16122237.0, 16188336.0, 16285752.0, 16473454.0, 16657981.0, 17383111.0, 17658147.0, 17711688.0, 17838568.0, 17949315.0, 18014095.0, 19404476.0, 20604456.0, 20705848.0, 20793935.0, 20906994.0, 21218706.0, 21647354.0, 21900643.0, 22022424.0, 22290133.0, 22859081.0, 22957609.0, 23289758.0, 23307174.0, 23351075.0, 23523185.0, 23780550.0, 23936642.0, 24245850.0, 24376806.0, 25471014.0, 25581305.0, 26142587.0, 27196773.0, 27241495.0, 27495481.0, 27529111.0, 27912686.0, 28063861.0, 28075557.0, 28397130.0, 29041970.0, 29210467.0, 29217676.0, 29251501.0, 29272286.0, 29311113.0, 29381888.0, 29516613.0, 30129958.0, 30173861.0, 30237844.0, ...]\n",
      "8  A08_R30_10  [32747877.0, 33225378.0, 30222825.0, 30399463.0, 31816529.0, 26087705.0, 26101671.0, 27208246.0, 27281690.0, 28450917.0, 28681485.0, 28870531.0, 36786747.0, 36881973.0, 94360111.0, 64611186.0, 65928170.0, 86555910.0, 87283052.0, 90142629.0, 92475377.0, 24211550.0, 18588946.0, 56909280.0, 49361421.0, 50740160.0, 51172131.0, 52830174.0, 19382071.0, 19386439.0, 19437585.0, 20580512.0, 60398990.0, 61039919.0, 62174101.0, 13506428.0, 14003070.0, 14905625.0, 58750698.0, 59173495.0, 70113334.0, 72432483.0, 72826556.0, 77348478.0, 77702417.0, 77895892.0, 78585663.0, 78955801.0, 82759759.0, 82761814.0, 82998036.0, 83766264.0, 85376389.0, 86041444.0, 39578614.0, 11310199.0, 11477598.0, 11992004.0, 12572706.0, 12825451.0, 13230085.0, 13272858.0, 13488430.0, 13802581.0, 13904150.0, 13949283.0, 14143116.0, 14566519.0, 14663910.0, 15061248.0, 15569979.0, 15652754.0, 15772762.0, 16057722.0, 16122237.0, 16188336.0, 16285752.0, 16473454.0, 16657981.0, 17383111.0, 17658147.0, 17711688.0, 17838568.0, 17949315.0, 18014095.0, 19404476.0, 20604456.0, 20705848.0, 20793935.0, 20906994.0, 21218706.0, 21647354.0, 21900643.0, 22022424.0, 22290133.0, 22859081.0, 22957609.0, 23289758.0, 23307174.0, 23351075.0, ...]\n",
      "9  A08_R31_10  [15935304.0, 16864844.0, 17412726.0, 18995900.0, 11091117.0, 13890443.0, 14218674.0, 14503146.0, 14507837.0, 14905625.0, 15375585.0, 15492350.0, 19495845.0, 19588805.0, 20906994.0, 21609439.0, 21976938.0, 22022424.0, 22522911.0, 60295353.0, 60313250.0, 60789509.0, 60952999.0, 61039919.0, 61496524.0, 61996093.0, 62081251.0, 62081755.0, 62174101.0, 63000751.0, 22949243.0, 23780550.0, 23960986.0, 24233742.0, 24442476.0, 24835901.0, 25144252.0, 25507118.0, 25871243.0, 26015123.0, 32704047.0, 33211728.0, 33443937.0, 33733609.0, 33972118.0, 34449127.0, 34664288.0, 35844572.0, 67497842.0, 68615057.0, 68934714.0, 69543031.0, 93760372.0, 94076502.0, 94777554.0, 94828505.0, 95278301.0, 63638882.0, 63641063.0, 63748616.0, 63937928.0, 63953614.0, 64023602.0, 64531224.0, 65534910.0, 65707211.0, 65708203.0, 65928170.0, 66010275.0, 66448850.0, 50909828.0, 51059377.0, 51172131.0, 51464686.0, 51537585.0, 52676975.0, 53488122.0, 53622851.0, 48543545.0, 48732141.0, 54079889.0, 54838031.0, 55228413.0, 55276077.0, 56409650.0, 76784921.0, 77948293.0, 78585663.0, 79125483.0, 79700784.0, 26101671.0, 26458046.0, 27277829.0, 27516718.0, 28450917.0, 28660685.0, 86993125.0, 87098124.0, 87651493.0, 87825072.0, ...]\n",
      "\n",
      "================================================================================\n",
      "데이터 타입:\n",
      "================================================================================\n",
      "key       object\n",
      "values    object\n",
      "dtype: object\n",
      "\n",
      "================================================================================\n",
      "기본 통계 정보:\n",
      "================================================================================\n",
      "               key                                             values\n",
      "count        12177                                              12177\n",
      "unique       12177                                               8177\n",
      "top     A08_R19_10  [40732020.0, 40828648.0, 41221658.0, 41501477....\n",
      "freq             1                                                250\n"
     ]
    }
   ],
   "source": [
    "# edge_pids_mapping_10.parquet 파일의 일부분 확인\n",
    "\n",
    "file_path = '/home/hashjamm/results/disease_network/hr_mapping_results_fu10/edge_pids_mapping_10.parquet'\n",
    "\n",
    "# 파일 존재 확인\n",
    "if os.path.exists(file_path):\n",
    "    # Parquet 파일 읽기 (엔진 명시)\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "    except Exception as e:\n",
    "        print(f\"pyarrow 엔진 실패: {e}\")\n",
    "        print(\"fastparquet 엔진으로 재시도...\")\n",
    "        df = pd.read_parquet(file_path, engine='fastparquet')\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"edge_pids_mapping_10.parquet 파일 정보\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\n전체 행 수: {len(df):,}개\")\n",
    "    print(f\"칼럼 수: {len(df.columns)}개\")\n",
    "    print(f\"\\n칼럼 목록:\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"  {i}. {col}\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"상위 10개 행:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(df.head(10).to_string())\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"데이터 타입:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"기본 통계 정보:\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(df.describe())\n",
    "    \n",
    "else:\n",
    "    print(f\"⚠️ 파일이 존재하지 않습니다: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7538b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/hashjamm/results/disease_network/edge_pids/edge_pids_1.pkl', 'rb') as f:\n",
    "    edge_pids_1 = pickle.load(f)\n",
    "\n",
    "outcome_dt_year_1 = pd.read_sas('/home/hashjamm/project_data/disease_network/sas_files/hr_project/outcome_dt_year_1.sas7bdat')\n",
    "outcome_dt_year_1 = outcome_dt_year_1.astype({\n",
    "    'PERSON_ID': 'int64',\n",
    "    'KEY_SEQ': 'string',\n",
    "    'abb_sick': 'string'\n",
    "})\n",
    "\n",
    "matched_a01 = pd.read_sas('/home/hashjamm/project_data/disease_network/sas_files/hr_project/matched_date/matched_a01.sas7bdat')\n",
    "matched_a01 = matched_a01.astype({\n",
    "    'PERSON_ID': 'int64',\n",
    "    'matched_id': 'int64',\n",
    "    'case': 'int64',\n",
    "    'index_date': 'string',\n",
    "    'DTH_YM': 'string',\n",
    "    'DTH_CODE1': 'string',\n",
    "    'DTH_CODE2': 'string',\n",
    "    'index_key_seq': 'string'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c20bcc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERSON_ID</th>\n",
       "      <th>matched_id</th>\n",
       "      <th>case</th>\n",
       "      <th>index_date</th>\n",
       "      <th>DTH_YM</th>\n",
       "      <th>DTH_CODE1</th>\n",
       "      <th>DTH_CODE2</th>\n",
       "      <th>index_key_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10241095</td>\n",
       "      <td>60744825</td>\n",
       "      <td>0</td>\n",
       "      <td>20030210</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10351989</td>\n",
       "      <td>31557339</td>\n",
       "      <td>0</td>\n",
       "      <td>20030429</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10684534</td>\n",
       "      <td>47160831</td>\n",
       "      <td>0</td>\n",
       "      <td>20031113</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11165309</td>\n",
       "      <td>74646440</td>\n",
       "      <td>0</td>\n",
       "      <td>20030814</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13706300</td>\n",
       "      <td>83310727</td>\n",
       "      <td>0</td>\n",
       "      <td>20030331</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>82895667</td>\n",
       "      <td>82895667</td>\n",
       "      <td>1</td>\n",
       "      <td>20030825</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>200309126976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>83310727</td>\n",
       "      <td>83310727</td>\n",
       "      <td>1</td>\n",
       "      <td>20030331</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>200304487730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>84546077</td>\n",
       "      <td>84546077</td>\n",
       "      <td>1</td>\n",
       "      <td>20030728</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>200307328399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>86072772</td>\n",
       "      <td>86072772</td>\n",
       "      <td>1</td>\n",
       "      <td>20030722</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>200300702841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>87760206</td>\n",
       "      <td>87760206</td>\n",
       "      <td>1</td>\n",
       "      <td>20030331</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>200303647857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PERSON_ID  matched_id  case index_date DTH_YM DTH_CODE1 DTH_CODE2  \\\n",
       "0     10241095    60744825     0   20030210   <NA>      <NA>      <NA>   \n",
       "1     10351989    31557339     0   20030429   <NA>      <NA>      <NA>   \n",
       "2     10684534    47160831     0   20031113   <NA>      <NA>      <NA>   \n",
       "3     11165309    74646440     0   20030814   <NA>      <NA>      <NA>   \n",
       "4     13706300    83310727     0   20030331   <NA>      <NA>      <NA>   \n",
       "..         ...         ...   ...        ...    ...       ...       ...   \n",
       "175   82895667    82895667     1   20030825   <NA>      <NA>      <NA>   \n",
       "176   83310727    83310727     1   20030331   <NA>      <NA>      <NA>   \n",
       "177   84546077    84546077     1   20030728   <NA>      <NA>      <NA>   \n",
       "178   86072772    86072772     1   20030722   <NA>      <NA>      <NA>   \n",
       "179   87760206    87760206     1   20030331   <NA>      <NA>      <NA>   \n",
       "\n",
       "    index_key_seq  \n",
       "0            <NA>  \n",
       "1            <NA>  \n",
       "2            <NA>  \n",
       "3            <NA>  \n",
       "4            <NA>  \n",
       "..            ...  \n",
       "175  200309126976  \n",
       "176  200304487730  \n",
       "177  200307328399  \n",
       "178  200300702841  \n",
       "179  200303647857  \n",
       "\n",
       "[180 rows x 8 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_a01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5543b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cause_abb_chunk(chunk_data):\n",
    "    \"\"\"\n",
    "    각 cause_abb 청크를 처리하는 함수\n",
    "    \n",
    "    Args:\n",
    "        chunk_data: 튜플 (cause_abbs_chunk, edge_pids_path, outcome_dt_year_path, \n",
    "                          matched_date_dir, chunk_idx, output_dir, fu)\n",
    "    \"\"\"\n",
    "    (cause_abbs_chunk, edge_pids_path, outcome_dt_year_path, matched_date_dir, \n",
    "     chunk_idx, output_dir, fu) = chunk_data\n",
    "    \n",
    "    # edge_pids 데이터 로드\n",
    "    with open(edge_pids_path, 'rb') as f:\n",
    "        edge_pids_dict = pickle.load(f)\n",
    "    \n",
    "    # outcome_dt_year 데이터 로드 및 타입 변환\n",
    "    outcome_dt_year_df = pd.read_sas(outcome_dt_year_path)\n",
    "    outcome_dt_year_df = outcome_dt_year_df.astype({\n",
    "        'PERSON_ID': 'int64',\n",
    "        'KEY_SEQ': 'string',\n",
    "        'abb_sick': 'string'\n",
    "    })\n",
    "    \n",
    "    # 인덱스 설정\n",
    "    outcome_index = outcome_dt_year_df.set_index(['PERSON_ID', 'abb_sick'])\n",
    "    \n",
    "    edge_index_key_seq_dict = {}\n",
    "    edge_key_seq_dict = {}\n",
    "    \n",
    "    for cause_abb in cause_abbs_chunk:\n",
    "        matched_file_path = os.path.join(matched_date_dir, f'matched_{cause_abb.lower()}.sas7bdat')\n",
    "        \n",
    "        if not os.path.exists(matched_file_path):\n",
    "            continue\n",
    "        \n",
    "        # matched 파일 로드 및 타입 변환\n",
    "        matched_df = pd.read_sas(matched_file_path)\n",
    "        matched_df = matched_df.astype({\n",
    "            'PERSON_ID': 'int64',\n",
    "            'matched_id': 'int64',\n",
    "            'case': 'int64',\n",
    "            'index_date': 'string',\n",
    "            'DTH_YM': 'string',\n",
    "            'DTH_CODE1': 'string',\n",
    "            'DTH_CODE2': 'string',\n",
    "            'index_key_seq': 'string'\n",
    "        })\n",
    "        \n",
    "        matched_case1 = matched_df[matched_df['case'] == 1]\n",
    "        person_to_index_key_seq = dict(zip(matched_case1['PERSON_ID'], matched_case1['index_key_seq']))\n",
    "        \n",
    "        # matched_df 사용 완료 후 메모리에서 삭제\n",
    "        del matched_df, matched_case1\n",
    "        \n",
    "        cause_combos = [(c_abb, o_abb) for (c_abb, o_abb) in edge_pids_dict.keys() if c_abb == cause_abb]\n",
    "        \n",
    "        for (c_abb, o_abb) in cause_combos:\n",
    "            pids_array = edge_pids_dict[(c_abb, o_abb)]\n",
    "            key = f'{c_abb}_{o_abb}_{fu}'\n",
    "            \n",
    "            pids_list = pids_array.tolist() if isinstance(pids_array, np.ndarray) else list(pids_array)\n",
    "            \n",
    "            index_key_seqs = [person_to_index_key_seq[pid] for pid in pids_list if pid in person_to_index_key_seq]\n",
    "            if index_key_seqs:\n",
    "                edge_index_key_seq_dict[key] = index_key_seqs\n",
    "            \n",
    "            outcome_keys = pd.MultiIndex.from_tuples([(pid, o_abb) for pid in pids_list])\n",
    "            matching_keys = outcome_keys.intersection(outcome_index.index)\n",
    "            if len(matching_keys) > 0:\n",
    "                key_seqs = outcome_index.loc[matching_keys, 'KEY_SEQ'].tolist()\n",
    "                if key_seqs:\n",
    "                    edge_key_seq_dict[key] = key_seqs\n",
    "        \n",
    "        # person_to_index_key_seq 사용 완료 후 메모리에서 삭제\n",
    "        del person_to_index_key_seq\n",
    "    \n",
    "    # outcome_dt_year_df와 outcome_index 사용 완료 후 메모리에서 삭제\n",
    "    del outcome_dt_year_df, outcome_index\n",
    "    \n",
    "    # edge_pids_dict 사용 완료 후 메모리에서 삭제\n",
    "    del edge_pids_dict\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    if edge_index_key_seq_dict:\n",
    "        df_index = pd.DataFrame({\n",
    "            'key': list(edge_index_key_seq_dict.keys()),\n",
    "            'values': list(edge_index_key_seq_dict.values())\n",
    "        })\n",
    "        df_index.to_parquet(\n",
    "            os.path.join(output_dir, f'edge_index_key_seq_mapping_{fu}_{chunk_idx}.parquet'),\n",
    "            engine='fastparquet',\n",
    "            index=False\n",
    "        )\n",
    "        del df_index\n",
    "    \n",
    "    if edge_key_seq_dict:\n",
    "        df_key = pd.DataFrame({\n",
    "            'key': list(edge_key_seq_dict.keys()),\n",
    "            'values': list(edge_key_seq_dict.values())\n",
    "        })\n",
    "        df_key.to_parquet(\n",
    "            os.path.join(output_dir, f'edge_key_seq_mapping_{fu}_{chunk_idx}.parquet'),\n",
    "            engine='fastparquet',\n",
    "            index=False\n",
    "        )\n",
    "        del df_key\n",
    "    \n",
    "    # 메모리 부하를 줄이기 위해 파일만 저장하고 반환하지 않음\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b24964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunks(fu, edge_pids_path, outcome_dt_year_path, matched_date_dir, output_dir, n_cores=60):\n",
    "    \"\"\"\n",
    "    청크를 생성하고 병렬 처리하는 함수\n",
    "    \n",
    "    Args:\n",
    "        fu: follow-up 기간\n",
    "        edge_pids_path: edge_pids pickle 파일 경로\n",
    "        outcome_dt_year_path: outcome_dt_year SAS 파일 경로\n",
    "        matched_date_dir: matched 파일들이 있는 디렉토리 경로\n",
    "        output_dir: 출력 디렉토리 경로\n",
    "        n_cores: 사용할 CPU 코어 수 (기본값: 60)\n",
    "    \"\"\"\n",
    "    with open(edge_pids_path, 'rb') as f:\n",
    "        edge_pids_temp = pickle.load(f)\n",
    "        unique_cause_abbs = sorted(set([combo[0] for combo in edge_pids_temp.keys()]))\n",
    "        del edge_pids_temp\n",
    "    \n",
    "    chunk_size = max(1, len(unique_cause_abbs) // n_cores)\n",
    "    cause_abb_chunks = [unique_cause_abbs[i:i + chunk_size] for i in range(0, len(unique_cause_abbs), chunk_size)]\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    chunk_data_list = [\n",
    "        (chunk, edge_pids_path, outcome_dt_year_path, matched_date_dir, idx + 1, output_dir, fu) \n",
    "        for idx, chunk in enumerate(cause_abb_chunks)\n",
    "    ]\n",
    "    \n",
    "    with Pool(processes=n_cores) as pool:\n",
    "        list(tqdm(\n",
    "            pool.imap(process_cause_abb_chunk, chunk_data_list),\n",
    "            total=len(chunk_data_list),\n",
    "            desc=\"Processing chunks\"\n",
    "        ))\n",
    "    \n",
    "    print(f\"모든 청크 처리 완료. 결과 파일은 {output_dir}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5208a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_chunk_files(fu, output_dir, delete_chunks=False):\n",
    "    \"\"\"\n",
    "    청크별로 나눠진 parquet 파일들을 합치는 함수\n",
    "    \n",
    "    Args:\n",
    "        fu: follow-up 기간\n",
    "        output_dir: 출력 디렉토리 경로\n",
    "        delete_chunks: 합친 후 기존 청크 파일들을 삭제할지 여부 (기본값: False)\n",
    "    \"\"\"\n",
    "    edge_index_files = sorted(glob.glob(os.path.join(output_dir, f'edge_index_key_seq_mapping_{fu}_*.parquet')))\n",
    "    edge_key_files = sorted(glob.glob(os.path.join(output_dir, f'edge_key_seq_mapping_{fu}_*.parquet')))\n",
    "    \n",
    "    print(f\"edge_index_key_seq_mapping 파일 개수: {len(edge_index_files)}개\")\n",
    "    print(f\"edge_key_seq_mapping 파일 개수: {len(edge_key_files)}개\")\n",
    "    \n",
    "    if edge_index_files:\n",
    "        print(f\"\\nedge_index_key_seq_mapping 파일 합치는 중...\")\n",
    "        edge_index_dfs = []\n",
    "        for file_path in tqdm(edge_index_files, desc=\"Loading edge_index files\"):\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "            except:\n",
    "                df = pd.read_parquet(file_path, engine='fastparquet')\n",
    "            edge_index_dfs.append(df)\n",
    "        \n",
    "        edge_index_merged = pd.concat(edge_index_dfs, ignore_index=True)\n",
    "        edge_index_output_path = os.path.join(output_dir, f'edge_index_key_seq_mapping_{fu}_dt.parquet')\n",
    "        edge_index_merged.to_parquet(edge_index_output_path, engine='fastparquet', index=False)\n",
    "        print(f\"합친 파일 저장 완료: {edge_index_output_path}\")\n",
    "        print(f\"  총 행 수: {len(edge_index_merged):,}개\")\n",
    "        del edge_index_merged, edge_index_dfs\n",
    "    \n",
    "    if edge_key_files:\n",
    "        print(f\"\\nedge_key_seq_mapping 파일 합치는 중...\")\n",
    "        edge_key_dfs = []\n",
    "        for file_path in tqdm(edge_key_files, desc=\"Loading edge_key files\"):\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "            except:\n",
    "                df = pd.read_parquet(file_path, engine='fastparquet')\n",
    "            edge_key_dfs.append(df)\n",
    "        \n",
    "        edge_key_merged = pd.concat(edge_key_dfs, ignore_index=True)\n",
    "        edge_key_output_path = os.path.join(output_dir, f'edge_key_seq_mapping_{fu}_dt.parquet')\n",
    "        edge_key_merged.to_parquet(edge_key_output_path, engine='fastparquet', index=False)\n",
    "        print(f\"합친 파일 저장 완료: {edge_key_output_path}\")\n",
    "        print(f\"  총 행 수: {len(edge_key_merged):,}개\")\n",
    "        del edge_key_merged, edge_key_dfs\n",
    "    \n",
    "    print(f\"\\n합치기 작업 완료!\")\n",
    "    \n",
    "    if delete_chunks:\n",
    "        print(f\"\\n기존 청크 파일들 삭제 중...\")\n",
    "        deleted_count = 0\n",
    "        for file_path in edge_index_files + edge_key_files:\n",
    "            try:\n",
    "                os.remove(file_path)\n",
    "                deleted_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"삭제 실패: {file_path} - {e}\")\n",
    "        print(f\"삭제 완료: {deleted_count}개 파일\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fu = 1\n",
    "edge_pids_path = '/home/hashjamm/results/disease_network/edge_pids/edge_pids_1.pkl'\n",
    "outcome_dt_year_path = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/outcome_dt_year_1.sas7bdat'\n",
    "matched_date_dir = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/matched_date'\n",
    "output_dir = '/home/hashjamm/results/disease_network/rr_mapping_results_fu1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36744704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 63/63 [1:22:59<00:00, 79.05s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 청크 처리 완료. 결과 파일은 /home/hashjamm/results/disease_network/rr_mapping_results_fu1에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "process_chunks(fu, edge_pids_path, outcome_dt_year_path, matched_date_dir, output_dir, n_cores=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650268f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index_key_seq_mapping 파일 개수: 63개\n",
      "edge_key_seq_mapping 파일 개수: 63개\n",
      "\n",
      "edge_index_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_index files: 100%|██████████| 63/63 [00:01<00:00, 49.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu1/edge_index_key_seq_mapping_1.parquet\n",
      "  총 행 수: 367,932개\n",
      "\n",
      "edge_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_key files: 100%|██████████| 63/63 [00:01<00:00, 54.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu1/edge_key_seq_mapping_1.parquet\n",
      "  총 행 수: 367,932개\n",
      "\n",
      "합치기 작업 완료!\n"
     ]
    }
   ],
   "source": [
    "merge_chunk_files(fu, output_dir, delete_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26ab1535",
   "metadata": {},
   "outputs": [],
   "source": [
    "fu = 2\n",
    "edge_pids_path = '/home/hashjamm/results/disease_network/edge_pids/edge_pids_2.pkl'\n",
    "outcome_dt_year_path = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/outcome_dt_year_2.sas7bdat'\n",
    "matched_date_dir = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/matched_date'\n",
    "output_dir = '/home/hashjamm/results/disease_network/rr_mapping_results_fu2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68094a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 92/92 [1:00:36<00:00, 39.53s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 청크 처리 완료. 결과 파일은 /home/hashjamm/results/disease_network/rr_mapping_results_fu2에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "process_chunks(fu, edge_pids_path, outcome_dt_year_path, matched_date_dir, output_dir, n_cores=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eec8917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index_key_seq_mapping 파일 개수: 92개\n",
      "edge_key_seq_mapping 파일 개수: 92개\n",
      "\n",
      "edge_index_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_index files: 100%|██████████| 92/92 [00:01<00:00, 84.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu2/edge_index_key_seq_mapping_2_dt.parquet\n",
      "  총 행 수: 341,239개\n",
      "\n",
      "edge_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_key files: 100%|██████████| 92/92 [00:00<00:00, 103.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu2/edge_key_seq_mapping_2_dt.parquet\n",
      "  총 행 수: 341,239개\n",
      "\n",
      "합치기 작업 완료!\n",
      "\n",
      "기존 청크 파일들 삭제 중...\n",
      "삭제 완료: 184개 파일\n"
     ]
    }
   ],
   "source": [
    "merge_chunk_files(fu, output_dir, delete_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592080bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fu = 3\n",
    "edge_pids_path = '/home/hashjamm/results/disease_network/edge_pids/edge_pids_3.pkl'\n",
    "outcome_dt_year_path = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/outcome_dt_year_3.sas7bdat'\n",
    "matched_date_dir = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/matched_date'\n",
    "output_dir = '/home/hashjamm/results/disease_network/rr_mapping_results_fu3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8c8592",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 63/63 [53:10<00:00, 50.65s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 청크 처리 완료. 결과 파일은 /home/hashjamm/results/disease_network/rr_mapping_results_fu3에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "process_chunks(fu, edge_pids_path, outcome_dt_year_path, matched_date_dir, output_dir, n_cores=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "713ff713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index_key_seq_mapping 파일 개수: 63개\n",
      "edge_key_seq_mapping 파일 개수: 63개\n",
      "\n",
      "edge_index_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_index files: 100%|██████████| 63/63 [00:00<00:00, 68.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu3/edge_index_key_seq_mapping_3.parquet\n",
      "  총 행 수: 332,967개\n",
      "\n",
      "edge_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_key files: 100%|██████████| 63/63 [00:00<00:00, 75.88it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu3/edge_key_seq_mapping_3.parquet\n",
      "  총 행 수: 332,967개\n",
      "\n",
      "합치기 작업 완료!\n",
      "\n",
      "기존 청크 파일들 삭제 중...\n",
      "삭제 완료: 126개 파일\n"
     ]
    }
   ],
   "source": [
    "merge_chunk_files(fu, output_dir, delete_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b22d9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fu = 4\n",
    "edge_pids_path = '/home/hashjamm/results/disease_network/edge_pids/edge_pids_4.pkl'\n",
    "outcome_dt_year_path = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/outcome_dt_year_4.sas7bdat'\n",
    "matched_date_dir = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/matched_date'\n",
    "output_dir = '/home/hashjamm/results/disease_network/rr_mapping_results_fu4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d03cd9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 92/92 [53:27<00:00, 34.86s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 청크 처리 완료. 결과 파일은 /home/hashjamm/results/disease_network/rr_mapping_results_fu4에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "process_chunks(fu, edge_pids_path, outcome_dt_year_path, matched_date_dir, output_dir, n_cores=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fd1cdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index_key_seq_mapping 파일 개수: 92개\n",
      "edge_key_seq_mapping 파일 개수: 92개\n",
      "\n",
      "edge_index_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_index files: 100%|██████████| 92/92 [00:00<00:00, 96.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu4/edge_index_key_seq_mapping_4.parquet\n",
      "  총 행 수: 335,706개\n",
      "\n",
      "edge_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_key files: 100%|██████████| 92/92 [00:00<00:00, 103.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu4/edge_key_seq_mapping_4.parquet\n",
      "  총 행 수: 335,706개\n",
      "\n",
      "합치기 작업 완료!\n",
      "\n",
      "기존 청크 파일들 삭제 중...\n",
      "삭제 완료: 184개 파일\n"
     ]
    }
   ],
   "source": [
    "merge_chunk_files(fu, output_dir, delete_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b85c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "fu = 5\n",
    "edge_pids_path = '/home/hashjamm/results/disease_network/edge_pids/edge_pids_5.pkl'\n",
    "outcome_dt_year_path = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/outcome_dt_year_5.sas7bdat'\n",
    "matched_date_dir = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/matched_date'\n",
    "output_dir = '/home/hashjamm/results/disease_network/rr_mapping_results_fu5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3523b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 92/92 [50:29<00:00, 32.93s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 청크 처리 완료. 결과 파일은 /home/hashjamm/results/disease_network/rr_mapping_results_fu5에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "process_chunks(fu, edge_pids_path, outcome_dt_year_path, matched_date_dir, output_dir, n_cores=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae1a2291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index_key_seq_mapping 파일 개수: 92개\n",
      "edge_key_seq_mapping 파일 개수: 92개\n",
      "\n",
      "edge_index_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_index files: 100%|██████████| 92/92 [00:00<00:00, 114.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu5/edge_index_key_seq_mapping_5.parquet\n",
      "  총 행 수: 331,601개\n",
      "\n",
      "edge_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_key files: 100%|██████████| 92/92 [00:00<00:00, 111.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu5/edge_key_seq_mapping_5.parquet\n",
      "  총 행 수: 331,601개\n",
      "\n",
      "합치기 작업 완료!\n",
      "\n",
      "기존 청크 파일들 삭제 중...\n",
      "삭제 완료: 184개 파일\n"
     ]
    }
   ],
   "source": [
    "merge_chunk_files(fu, output_dir, delete_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "791b3cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fu = 6\n",
    "edge_pids_path = '/home/hashjamm/results/disease_network/edge_pids/edge_pids_6.pkl'\n",
    "outcome_dt_year_path = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/outcome_dt_year_6.sas7bdat'\n",
    "matched_date_dir = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/matched_date'\n",
    "output_dir = '/home/hashjamm/results/disease_network/rr_mapping_results_fu6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b419694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 92/92 [50:19<00:00, 32.82s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 청크 처리 완료. 결과 파일은 /home/hashjamm/results/disease_network/rr_mapping_results_fu6에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "process_chunks(fu, edge_pids_path, outcome_dt_year_path, matched_date_dir, output_dir, n_cores=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed931f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index_key_seq_mapping 파일 개수: 92개\n",
      "edge_key_seq_mapping 파일 개수: 92개\n",
      "\n",
      "edge_index_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_index files: 100%|██████████| 92/92 [00:00<00:00, 114.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu6/edge_index_key_seq_mapping_6.parquet\n",
      "  총 행 수: 328,097개\n",
      "\n",
      "edge_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_key files: 100%|██████████| 92/92 [00:00<00:00, 121.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu6/edge_key_seq_mapping_6.parquet\n",
      "  총 행 수: 328,097개\n",
      "\n",
      "합치기 작업 완료!\n",
      "\n",
      "기존 청크 파일들 삭제 중...\n",
      "삭제 완료: 184개 파일\n"
     ]
    }
   ],
   "source": [
    "merge_chunk_files(fu, output_dir, delete_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6ec18c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fu = 7\n",
    "edge_pids_path = '/home/hashjamm/results/disease_network/edge_pids/edge_pids_7.pkl'\n",
    "outcome_dt_year_path = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/outcome_dt_year_7.sas7bdat'\n",
    "matched_date_dir = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/matched_date'\n",
    "output_dir = '/home/hashjamm/results/disease_network/rr_mapping_results_fu7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73b2d427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/92 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 92/92 [45:01<00:00, 29.36s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 청크 처리 완료. 결과 파일은 /home/hashjamm/results/disease_network/rr_mapping_results_fu7에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "process_chunks(fu, edge_pids_path, outcome_dt_year_path, matched_date_dir, output_dir, n_cores=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b4c5fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index_key_seq_mapping 파일 개수: 92개\n",
      "edge_key_seq_mapping 파일 개수: 92개\n",
      "\n",
      "edge_index_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_index files: 100%|██████████| 92/92 [00:00<00:00, 120.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu7/edge_index_key_seq_mapping_7.parquet\n",
      "  총 행 수: 323,245개\n",
      "\n",
      "edge_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_key files: 100%|██████████| 92/92 [00:00<00:00, 128.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu7/edge_key_seq_mapping_7.parquet\n",
      "  총 행 수: 323,245개\n",
      "\n",
      "합치기 작업 완료!\n",
      "\n",
      "기존 청크 파일들 삭제 중...\n",
      "삭제 완료: 184개 파일\n"
     ]
    }
   ],
   "source": [
    "merge_chunk_files(fu, output_dir, delete_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "346b1147",
   "metadata": {},
   "outputs": [],
   "source": [
    "fu = 8\n",
    "edge_pids_path = '/home/hashjamm/results/disease_network/edge_pids/edge_pids_8.pkl'\n",
    "outcome_dt_year_path = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/outcome_dt_year_8.sas7bdat'\n",
    "matched_date_dir = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/matched_date'\n",
    "output_dir = '/home/hashjamm/results/disease_network/rr_mapping_results_fu8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7bc42b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/92 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 92/92 [49:40<00:00, 32.40s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 청크 처리 완료. 결과 파일은 /home/hashjamm/results/disease_network/rr_mapping_results_fu8에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "process_chunks(fu, edge_pids_path, outcome_dt_year_path, matched_date_dir, output_dir, n_cores=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21b9f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index_key_seq_mapping 파일 개수: 92개\n",
      "edge_key_seq_mapping 파일 개수: 92개\n",
      "\n",
      "edge_index_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_index files: 100%|██████████| 92/92 [00:00<00:00, 114.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu8/edge_index_key_seq_mapping_8.parquet\n",
      "  총 행 수: 330,057개\n",
      "\n",
      "edge_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_key files: 100%|██████████| 92/92 [00:00<00:00, 121.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu8/edge_key_seq_mapping_8.parquet\n",
      "  총 행 수: 330,057개\n",
      "\n",
      "합치기 작업 완료!\n",
      "\n",
      "기존 청크 파일들 삭제 중...\n",
      "삭제 완료: 184개 파일\n"
     ]
    }
   ],
   "source": [
    "merge_chunk_files(fu, output_dir, delete_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f97208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fu = 9\n",
    "edge_pids_path = '/home/hashjamm/results/disease_network/edge_pids/edge_pids_9.pkl'\n",
    "outcome_dt_year_path = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/outcome_dt_year_9.sas7bdat'\n",
    "matched_date_dir = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/matched_date'\n",
    "output_dir = '/home/hashjamm/results/disease_network/rr_mapping_results_fu9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "561322b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/92 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 92/92 [47:16<00:00, 30.84s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 청크 처리 완료. 결과 파일은 /home/hashjamm/results/disease_network/rr_mapping_results_fu9에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "process_chunks(fu, edge_pids_path, outcome_dt_year_path, matched_date_dir, output_dir, n_cores=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f19014d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index_key_seq_mapping 파일 개수: 92개\n",
      "edge_key_seq_mapping 파일 개수: 92개\n",
      "\n",
      "edge_index_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_index files: 100%|██████████| 92/92 [00:00<00:00, 109.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu9/edge_index_key_seq_mapping_9.parquet\n",
      "  총 행 수: 326,263개\n",
      "\n",
      "edge_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_key files: 100%|██████████| 92/92 [00:00<00:00, 107.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu9/edge_key_seq_mapping_9.parquet\n",
      "  총 행 수: 326,263개\n",
      "\n",
      "합치기 작업 완료!\n",
      "\n",
      "기존 청크 파일들 삭제 중...\n",
      "삭제 완료: 184개 파일\n"
     ]
    }
   ],
   "source": [
    "merge_chunk_files(fu, output_dir, delete_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d13e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fu = 10\n",
    "edge_pids_path = '/home/hashjamm/results/disease_network/edge_pids/edge_pids_10.pkl'\n",
    "outcome_dt_year_path = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/outcome_dt_year_10.sas7bdat'\n",
    "matched_date_dir = '/home/hashjamm/project_data/disease_network/sas_files/hr_project/matched_date'\n",
    "output_dir = '/home/hashjamm/results/disease_network/rr_mapping_results_fu10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8274c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/92 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 92/92 [41:55<00:00, 27.34s/it]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 청크 처리 완료. 결과 파일은 /home/hashjamm/results/disease_network/rr_mapping_results_fu10에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "process_chunks(fu, edge_pids_path, outcome_dt_year_path, matched_date_dir, output_dir, n_cores=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "705f8b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index_key_seq_mapping 파일 개수: 92개\n",
      "edge_key_seq_mapping 파일 개수: 92개\n",
      "\n",
      "edge_index_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_index files: 100%|██████████| 92/92 [00:00<00:00, 112.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu10/edge_index_key_seq_mapping_10.parquet\n",
      "  총 행 수: 320,428개\n",
      "\n",
      "edge_key_seq_mapping 파일 합치는 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading edge_key files: 100%|██████████| 92/92 [00:00<00:00, 119.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "합친 파일 저장 완료: /home/hashjamm/results/disease_network/rr_mapping_results_fu10/edge_key_seq_mapping_10.parquet\n",
      "  총 행 수: 320,428개\n",
      "\n",
      "합치기 작업 완료!\n",
      "\n",
      "기존 청크 파일들 삭제 중...\n",
      "삭제 완료: 184개 파일\n"
     ]
    }
   ],
   "source": [
    "merge_chunk_files(fu, output_dir, delete_chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72378666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting fu=1: 100%|██████████| 367932/367932 [00:00<00:00, 505095.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fu=1: 367,932개 조합 저장 완료 -> /home/hashjamm/results/disease_network/rr_mapping_results_fu1/edge_pids_mapping_1_dt.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting fu=2: 100%|██████████| 341239/341239 [00:01<00:00, 325470.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fu=2: 341,239개 조합 저장 완료 -> /home/hashjamm/results/disease_network/rr_mapping_results_fu2/edge_pids_mapping_2_dt.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting fu=3: 100%|██████████| 332967/332967 [00:01<00:00, 298493.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fu=3: 332,967개 조합 저장 완료 -> /home/hashjamm/results/disease_network/rr_mapping_results_fu3/edge_pids_mapping_3_dt.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting fu=4: 100%|██████████| 335706/335706 [00:00<00:00, 346554.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fu=4: 335,706개 조합 저장 완료 -> /home/hashjamm/results/disease_network/rr_mapping_results_fu4/edge_pids_mapping_4_dt.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting fu=5: 100%|██████████| 331601/331601 [00:01<00:00, 304498.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fu=5: 331,601개 조합 저장 완료 -> /home/hashjamm/results/disease_network/rr_mapping_results_fu5/edge_pids_mapping_5_dt.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting fu=6: 100%|██████████| 328097/328097 [00:00<00:00, 369660.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fu=6: 328,097개 조합 저장 완료 -> /home/hashjamm/results/disease_network/rr_mapping_results_fu6/edge_pids_mapping_6_dt.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting fu=7: 100%|██████████| 323245/323245 [00:01<00:00, 308142.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fu=7: 323,245개 조합 저장 완료 -> /home/hashjamm/results/disease_network/rr_mapping_results_fu7/edge_pids_mapping_7_dt.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting fu=8: 100%|██████████| 330057/330057 [00:00<00:00, 366021.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fu=8: 330,057개 조합 저장 완료 -> /home/hashjamm/results/disease_network/rr_mapping_results_fu8/edge_pids_mapping_8_dt.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting fu=9: 100%|██████████| 326263/326263 [00:00<00:00, 403740.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fu=9: 326,263개 조합 저장 완료 -> /home/hashjamm/results/disease_network/rr_mapping_results_fu9/edge_pids_mapping_9_dt.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting fu=10: 100%|██████████| 320428/320428 [00:00<00:00, 354954.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fu=10: 320,428개 조합 저장 완료 -> /home/hashjamm/results/disease_network/rr_mapping_results_fu10/edge_pids_mapping_10_dt.parquet\n",
      "\n",
      "모든 변환 작업 완료!\n"
     ]
    }
   ],
   "source": [
    "def convert_edge_pids_to_parquet(fu, edge_pids_path, output_dir):\n",
    "    \"\"\"\n",
    "    edge_pids pkl 파일을 mapping parquet 형식으로 변환하는 함수\n",
    "    \n",
    "    Args:\n",
    "        fu: follow-up 기간\n",
    "        edge_pids_path: edge_pids pickle 파일 경로\n",
    "        output_dir: 출력 디렉토리 경로\n",
    "    \"\"\"\n",
    "    with open(edge_pids_path, 'rb') as f:\n",
    "        edge_pids = pickle.load(f)\n",
    "    \n",
    "    data_list = []\n",
    "    for (cause_abb, outcome_abb), pids_array in tqdm(edge_pids.items(), desc=f\"Converting fu={fu}\"):\n",
    "        key = f'{cause_abb}_{outcome_abb}_{fu}'\n",
    "        pids_list = pids_array.tolist() if isinstance(pids_array, np.ndarray) else list(pids_array)\n",
    "        data_list.append({'key': key, 'values': pids_list})\n",
    "    \n",
    "    del edge_pids\n",
    "    \n",
    "    df = pd.DataFrame(data_list)\n",
    "    del data_list\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f'edge_pids_mapping_{fu}_dt.parquet')\n",
    "    df.to_parquet(output_path, engine='fastparquet', index=False)\n",
    "    \n",
    "    row_count = len(df)\n",
    "    del df\n",
    "    \n",
    "    print(f\"fu={fu}: {row_count:,}개 조합 저장 완료 -> {output_path}\")\n",
    "\n",
    "edge_pids_base_dir = '/home/hashjamm/results/disease_network/edge_pids'\n",
    "output_base_dir = '/home/hashjamm/results/disease_network'\n",
    "\n",
    "for fu in range(1, 11):\n",
    "    edge_pids_path = os.path.join(edge_pids_base_dir, f'edge_pids_{fu}.pkl')\n",
    "    output_dir = os.path.join(output_base_dir, f'rr_mapping_results_fu{fu}')\n",
    "    \n",
    "    if os.path.exists(edge_pids_path):\n",
    "        convert_edge_pids_to_parquet(fu, edge_pids_path, output_dir)\n",
    "    else:\n",
    "        print(f\"⚠️ 파일이 존재하지 않습니다: {edge_pids_path}\")\n",
    "\n",
    "print(\"\\n모든 변환 작업 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df37c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "edge_index_key_seq_mapping 처리 시작\n",
      "================================================================================\n",
      "모든 dt 파일 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  dt 파일 로드: 100%|██████████| 10/10 [01:32<00:00,  9.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 fu에 대해 누적 매핑 데이터 생성 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  누적 데이터 생성: 100%|██████████| 10/10 [02:19<00:00, 13.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index_key_seq_mapping 처리 완료!\n",
      "\n",
      "================================================================================\n",
      "edge_key_seq_mapping 처리 시작\n",
      "================================================================================\n",
      "모든 dt 파일 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  dt 파일 로드: 100%|██████████| 10/10 [01:33<00:00,  9.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 fu에 대해 누적 매핑 데이터 생성 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  누적 데이터 생성: 100%|██████████| 10/10 [02:22<00:00, 14.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_key_seq_mapping 처리 완료!\n",
      "\n",
      "================================================================================\n",
      "edge_pids_mapping 처리 시작\n",
      "================================================================================\n",
      "모든 dt 파일 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  dt 파일 로드: 100%|██████████| 10/10 [01:31<00:00,  9.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 fu에 대해 누적 매핑 데이터 생성 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  누적 데이터 생성: 100%|██████████| 10/10 [02:03<00:00, 12.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_pids_mapping 처리 완료!\n",
      "\n",
      "모든 누적 매핑 데이터 생성 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def create_all_cumulative_mappings(base_dir='/home/hashjamm/results/disease_network'):\n",
    "    \"\"\"\n",
    "    모든 dt 파일을 메모리에 올린 후 각 fu에 대해 누적 매핑 데이터를 생성하는 함수\n",
    "    \n",
    "    Args:\n",
    "        base_dir: 기본 디렉토리 경로\n",
    "    \"\"\"\n",
    "    mapping_types = [\n",
    "        'edge_index_key_seq_mapping',\n",
    "        'edge_key_seq_mapping',\n",
    "        'edge_pids_mapping'\n",
    "    ]\n",
    "    \n",
    "    for mapping_type in mapping_types:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{mapping_type} 처리 시작\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        dt_data = {}\n",
    "        \n",
    "        print(f\"모든 dt 파일 로드 중...\")\n",
    "        for fu in tqdm(range(1, 11), desc=\"  dt 파일 로드\"):\n",
    "            source_dir = os.path.join(base_dir, f'rr_mapping_results_fu{fu}')\n",
    "            dt_file_path = os.path.join(source_dir, f'{mapping_type}_{fu}_dt.parquet')\n",
    "            \n",
    "            if not os.path.exists(dt_file_path):\n",
    "                dt_data[fu] = {}\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                df = pd.read_parquet(dt_file_path, engine='pyarrow')\n",
    "            except:\n",
    "                df = pd.read_parquet(dt_file_path, engine='fastparquet')\n",
    "            \n",
    "            fu_data = {}\n",
    "            for _, row in df.iterrows():\n",
    "                fu_data[row['key']] = row['values']\n",
    "            \n",
    "            dt_data[fu] = fu_data\n",
    "            del df\n",
    "        \n",
    "        print(f\"각 fu에 대해 누적 매핑 데이터 생성 중...\")\n",
    "        for target_fu in tqdm(range(1, 11), desc=\"  누적 데이터 생성\"):\n",
    "            output_dir = os.path.join(base_dir, f'rr_mapping_results_fu{target_fu}')\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "            merged_data = {}\n",
    "            \n",
    "            for current_fu in range(1, target_fu + 1):\n",
    "                if current_fu not in dt_data or not dt_data[current_fu]:\n",
    "                    continue\n",
    "                \n",
    "                for key, values in dt_data[current_fu].items():\n",
    "                    if key not in merged_data:\n",
    "                        merged_data[key] = []\n",
    "                    \n",
    "                    if isinstance(values, list):\n",
    "                        merged_data[key].extend(values)\n",
    "                    else:\n",
    "                        merged_data[key].append(values)\n",
    "            \n",
    "            if not merged_data:\n",
    "                continue\n",
    "            \n",
    "            result_list = []\n",
    "            for key, values_list in merged_data.items():\n",
    "                unique_values = list(set(values_list))\n",
    "                result_list.append({'key': key, 'values': unique_values})\n",
    "            \n",
    "            del merged_data\n",
    "            \n",
    "            result_df = pd.DataFrame(result_list)\n",
    "            del result_list\n",
    "            \n",
    "            output_path = os.path.join(output_dir, f'{mapping_type}_{target_fu}.parquet')\n",
    "            result_df.to_parquet(output_path, engine='fastparquet', index=False)\n",
    "            \n",
    "            del result_df\n",
    "        \n",
    "        del dt_data\n",
    "        print(f\"{mapping_type} 처리 완료!\")\n",
    "\n",
    "create_all_cumulative_mappings()\n",
    "\n",
    "print(\"\\n모든 누적 매핑 데이터 생성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab2501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fu=1 폴더 처리 중: /home/hashjamm/results/disease_network/rr_mapping_results_fu1\n",
      "  삭제: edge_index_key_seq_mapping_1_dt.parquet\n",
      "  삭제: edge_key_seq_mapping_1_dt.parquet\n",
      "  삭제: edge_pids_mapping_1_dt.parquet\n",
      "  총 3개 파일 삭제 완료\n",
      "\n",
      "fu=2 폴더 처리 중: /home/hashjamm/results/disease_network/rr_mapping_results_fu2\n",
      "  삭제: edge_index_key_seq_mapping_2_dt.parquet\n",
      "  삭제: edge_key_seq_mapping_2_dt.parquet\n",
      "  삭제: edge_pids_mapping_2_dt.parquet\n",
      "  총 3개 파일 삭제 완료\n",
      "\n",
      "fu=3 폴더 처리 중: /home/hashjamm/results/disease_network/rr_mapping_results_fu3\n",
      "  삭제: edge_index_key_seq_mapping_3_dt.parquet\n",
      "  삭제: edge_key_seq_mapping_3_dt.parquet\n",
      "  삭제: edge_pids_mapping_3_dt.parquet\n",
      "  총 3개 파일 삭제 완료\n",
      "\n",
      "fu=4 폴더 처리 중: /home/hashjamm/results/disease_network/rr_mapping_results_fu4\n",
      "  삭제: edge_index_key_seq_mapping_4_dt.parquet\n",
      "  삭제: edge_key_seq_mapping_4_dt.parquet\n",
      "  삭제: edge_pids_mapping_4_dt.parquet\n",
      "  총 3개 파일 삭제 완료\n",
      "\n",
      "fu=5 폴더 처리 중: /home/hashjamm/results/disease_network/rr_mapping_results_fu5\n",
      "  삭제: edge_index_key_seq_mapping_5_dt.parquet\n",
      "  삭제: edge_key_seq_mapping_5_dt.parquet\n",
      "  삭제: edge_pids_mapping_5_dt.parquet\n",
      "  총 3개 파일 삭제 완료\n",
      "\n",
      "fu=6 폴더 처리 중: /home/hashjamm/results/disease_network/rr_mapping_results_fu6\n",
      "  삭제: edge_index_key_seq_mapping_6_dt.parquet\n",
      "  삭제: edge_key_seq_mapping_6_dt.parquet\n",
      "  삭제: edge_pids_mapping_6_dt.parquet\n",
      "  총 3개 파일 삭제 완료\n",
      "\n",
      "fu=7 폴더 처리 중: /home/hashjamm/results/disease_network/rr_mapping_results_fu7\n",
      "  삭제: edge_index_key_seq_mapping_7_dt.parquet\n",
      "  삭제: edge_key_seq_mapping_7_dt.parquet\n",
      "  삭제: edge_pids_mapping_7_dt.parquet\n",
      "  총 3개 파일 삭제 완료\n",
      "\n",
      "fu=8 폴더 처리 중: /home/hashjamm/results/disease_network/rr_mapping_results_fu8\n",
      "  삭제: edge_index_key_seq_mapping_8_dt.parquet\n",
      "  삭제: edge_key_seq_mapping_8_dt.parquet\n",
      "  삭제: edge_pids_mapping_8_dt.parquet\n",
      "  총 3개 파일 삭제 완료\n",
      "\n",
      "fu=9 폴더 처리 중: /home/hashjamm/results/disease_network/rr_mapping_results_fu9\n",
      "  삭제: edge_index_key_seq_mapping_9_dt.parquet\n",
      "  삭제: edge_key_seq_mapping_9_dt.parquet\n",
      "  삭제: edge_pids_mapping_9_dt.parquet\n",
      "  총 3개 파일 삭제 완료\n",
      "\n",
      "fu=10 폴더 처리 중: /home/hashjamm/results/disease_network/rr_mapping_results_fu10\n",
      "  삭제: edge_index_key_seq_mapping_10_dt.parquet\n",
      "  삭제: edge_key_seq_mapping_10_dt.parquet\n",
      "  삭제: edge_pids_mapping_10_dt.parquet\n",
      "  총 3개 파일 삭제 완료\n",
      "\n",
      "================================================================================\n",
      "전체 삭제 완료: 총 30개 dt 파일 삭제됨\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def delete_dt_files(base_dir='/home/hashjamm/results/disease_network'):\n",
    "    \"\"\"\n",
    "    각 폴더별 dt 파일들을 삭제하는 함수\n",
    "    \n",
    "    Args:\n",
    "        base_dir: 기본 디렉토리 경로\n",
    "    \"\"\"\n",
    "    mapping_types = [\n",
    "        'edge_index_key_seq_mapping',\n",
    "        'edge_key_seq_mapping',\n",
    "        'edge_pids_mapping'\n",
    "    ]\n",
    "    \n",
    "    total_deleted = 0\n",
    "    \n",
    "    for fu in range(1, 11):\n",
    "        folder_path = os.path.join(base_dir, f'rr_mapping_results_fu{fu}')\n",
    "        \n",
    "        if not os.path.exists(folder_path):\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nfu={fu} 폴더 처리 중: {folder_path}\")\n",
    "        deleted_count = 0\n",
    "        \n",
    "        for mapping_type in mapping_types:\n",
    "            dt_file_pattern = os.path.join(folder_path, f'{mapping_type}_{fu}_dt.parquet')\n",
    "            \n",
    "            if os.path.exists(dt_file_pattern):\n",
    "                try:\n",
    "                    os.remove(dt_file_pattern)\n",
    "                    deleted_count += 1\n",
    "                    print(f\"  삭제: {os.path.basename(dt_file_pattern)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  삭제 실패: {os.path.basename(dt_file_pattern)} - {e}\")\n",
    "        \n",
    "        if deleted_count == 0:\n",
    "            print(f\"  dt 파일이 없습니다.\")\n",
    "        else:\n",
    "            print(f\"  총 {deleted_count}개 파일 삭제 완료\")\n",
    "            total_deleted += deleted_count\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"전체 삭제 완료: 총 {total_deleted}개 dt 파일 삭제됨\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "delete_dt_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208f565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disease_network_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
